[
  {
    "id" : "b144e53e-a1e1-4290-b390-091a5ea39cfa",
    "prId" : 1423,
    "comments" : [
      {
        "id" : "5f67447c-54a3-403f-8865-3d930be90b45",
        "parentId" : null,
        "author" : {
          "login" : "densh",
          "name" : null,
          "avatarUrl" : "https://avatars1.githubusercontent.com/u/320966?u=784f6f761f35b8b7f3f787172b468334d6524524&v=4"
        },
        "body" : "`Sweeper_LazySweep` needs a better name. Based on the code it's more like `Sweeper_LazySweepUntilCanAllocate`.",
        "createdAt" : "2019-02-01T10:02:07Z",
        "updatedAt" : "2019-07-17T09:53:34Z",
        "lastEditedBy" : {
          "login" : "densh",
          "name" : null,
          "avatarUrl" : "https://avatars1.githubusercontent.com/u/320966?u=784f6f761f35b8b7f3f787172b468334d6524524&v=4"
        },
        "tags" : [
          {
            "value" : "outdated"
          }
        ]
      },
      {
        "id" : "adbae8ed-bb89-4eea-91e3-c19fe35ee8b8",
        "parentId" : "5f67447c-54a3-403f-8865-3d930be90b45",
        "author" : {
          "login" : "valdisxp1",
          "name" : null,
          "avatarUrl" : "https://avatars3.githubusercontent.com/u/2277076?v=4"
        },
        "body" : "Sweeping until it is possible to allocate is literally the definition of lazy sweeping. `UntilCanAllocate` is redundant.",
        "createdAt" : "2019-02-01T20:35:00Z",
        "updatedAt" : "2019-07-17T09:53:34Z",
        "lastEditedBy" : {
          "login" : "valdisxp1",
          "name" : null,
          "avatarUrl" : "https://avatars3.githubusercontent.com/u/2277076?v=4"
        },
        "tags" : [
          {
            "value" : "outdated"
          }
        ]
      }
    ],
    "commit" : "9349177c2456e08c91598c5a158c63a30876a4e0",
    "line" : null,
    "diffHunk" : "@@ -0,0 +1,542 @@\n+#include <stdlib.h>\n+#include <sys/mman.h>\n+#include <stdio.h>\n+#include \"Heap.h\"\n+#include \"Log.h\"\n+#include \"Allocator.h\"\n+#include \"Marker.h\"\n+#include \"State.h\"\n+#include \"utils/MathUtils.h\"\n+#include \"StackTrace.h\"\n+#include \"Settings.h\"\n+#include \"Memory.h\"\n+#include \"GCThread.h\"\n+#include \"Sweeper.h\"\n+#include <memory.h>\n+#include <time.h>\n+#include <inttypes.h>\n+#include <sched.h>\n+\n+// Allow read and write\n+#define HEAP_MEM_PROT (PROT_READ | PROT_WRITE)\n+// Map private anonymous memory, and prevent from reserving swap\n+#define HEAP_MEM_FLAGS (MAP_NORESERVE | MAP_PRIVATE | MAP_ANONYMOUS)\n+// Map anonymous memory (not a file)\n+#define HEAP_MEM_FD -1\n+#define HEAP_MEM_FD_OFFSET 0\n+\n+void Heap_exitWithOutOfMemory() {\n+    printf(\"Out of heap space\\n\");\n+    StackTrace_PrintStackTrace();\n+    exit(1);\n+}\n+\n+bool Heap_isGrowingPossible(Heap *heap, uint32_t incrementInBlocks) {\n+    return heap->blockCount + incrementInBlocks <= heap->maxBlockCount;\n+}\n+\n+size_t Heap_getMemoryLimit() {\n+    size_t memorySize = getMemorySize();\n+    if ((uint64_t)memorySize > MAX_HEAP_SIZE) {\n+        return (size_t)MAX_HEAP_SIZE;\n+    } else {\n+        return memorySize;\n+    }\n+}\n+\n+/**\n+ * Maps `MAX_SIZE` of memory and returns the first address aligned on\n+ * `alignement` mask\n+ */\n+word_t *Heap_mapAndAlign(size_t memoryLimit, size_t alignmentSize) {\n+    assert(alignmentSize % WORD_SIZE == 0);\n+    word_t *heapStart = mmap(NULL, memoryLimit, HEAP_MEM_PROT, HEAP_MEM_FLAGS,\n+                             HEAP_MEM_FD, HEAP_MEM_FD_OFFSET);\n+\n+    size_t alignmentMask = ~(alignmentSize - 1);\n+    // Heap start not aligned on\n+    if (((word_t)heapStart & alignmentMask) != (word_t)heapStart) {\n+        word_t *previousBlock = (word_t *)((word_t)heapStart & alignmentMask);\n+        heapStart = previousBlock + alignmentSize / WORD_SIZE;\n+    }\n+    return heapStart;\n+}\n+\n+/**\n+ * Allocates the heap struct and initializes it\n+ */\n+void Heap_Init(Heap *heap, size_t minHeapSize, size_t maxHeapSize) {\n+    size_t memoryLimit = Heap_getMemoryLimit();\n+\n+    if (maxHeapSize < MIN_HEAP_SIZE) {\n+        fprintf(stderr,\n+                \"SCALANATIVE_MAX_HEAP_SIZE too small to initialize heap.\\n\");\n+        fprintf(stderr, \"Minimum required: %zum \\n\",\n+                MIN_HEAP_SIZE / 1024 / 1024);\n+        fflush(stderr);\n+        exit(1);\n+    }\n+\n+    if (minHeapSize > memoryLimit) {\n+        fprintf(stderr, \"SCALANATIVE_MIN_HEAP_SIZE is too large.\\n\");\n+        fprintf(stderr, \"Maximum possible: %zug \\n\",\n+                memoryLimit / 1024 / 1024 / 1024);\n+        fflush(stderr);\n+        exit(1);\n+    }\n+\n+    if (maxHeapSize < minHeapSize) {\n+        fprintf(stderr, \"SCALANATIVE_MAX_HEAP_SIZE should be at least \"\n+                        \"SCALANATIVE_MIN_HEAP_SIZE\\n\");\n+        fflush(stderr);\n+        exit(1);\n+    }\n+\n+    if (minHeapSize < MIN_HEAP_SIZE) {\n+        minHeapSize = MIN_HEAP_SIZE;\n+    }\n+\n+    if (maxHeapSize == UNLIMITED_HEAP_SIZE) {\n+        maxHeapSize = memoryLimit;\n+    }\n+\n+    uint32_t maxNumberOfBlocks = maxHeapSize / SPACE_USED_PER_BLOCK;\n+    uint32_t initialBlockCount = minHeapSize / SPACE_USED_PER_BLOCK;\n+    heap->maxHeapSize = maxHeapSize;\n+    heap->blockCount = initialBlockCount;\n+    heap->maxBlockCount = maxNumberOfBlocks;\n+\n+    // reserve space for block headers\n+    size_t blockMetaSpaceSize = maxNumberOfBlocks * sizeof(BlockMeta);\n+    word_t *blockMetaStart = Heap_mapAndAlign(blockMetaSpaceSize, WORD_SIZE);\n+    heap->blockMetaStart = blockMetaStart;\n+    heap->blockMetaEnd =\n+        blockMetaStart + initialBlockCount * sizeof(BlockMeta) / WORD_SIZE;\n+\n+    // reserve space for line headers\n+    size_t lineMetaSpaceSize =\n+        (size_t)maxNumberOfBlocks * LINE_COUNT * LINE_METADATA_SIZE;\n+    word_t *lineMetaStart = Heap_mapAndAlign(lineMetaSpaceSize, WORD_SIZE);\n+    heap->lineMetaStart = lineMetaStart;\n+    assert(LINE_COUNT * LINE_SIZE == BLOCK_TOTAL_SIZE);\n+    assert(LINE_COUNT * LINE_METADATA_SIZE % WORD_SIZE == 0);\n+    heap->lineMetaEnd = lineMetaStart + initialBlockCount * LINE_COUNT *\n+                                            LINE_METADATA_SIZE / WORD_SIZE;\n+\n+    word_t *heapStart = Heap_mapAndAlign(maxHeapSize, BLOCK_TOTAL_SIZE);\n+\n+    BlockAllocator_Init(&blockAllocator, blockMetaStart, initialBlockCount);\n+    GreyList_Init(&heap->mark.empty);\n+    GreyList_Init(&heap->mark.full);\n+    uint32_t greyPacketCount = (uint32_t)(maxHeapSize * GREY_PACKET_RATIO / GREY_PACKET_SIZE);\n+    heap->mark.total = greyPacketCount;\n+    word_t* greyPacketsStart = Heap_mapAndAlign(greyPacketCount * sizeof(GreyPacket), WORD_SIZE);\n+    heap->greyPacketsStart = greyPacketsStart;\n+    GreyList_PushAll(&heap->mark.empty, greyPacketsStart, (GreyPacket *) greyPacketsStart, greyPacketCount);\n+\n+    // reserve space for bytemap\n+    Bytemap *bytemap = (Bytemap *)Heap_mapAndAlign(\n+        maxHeapSize / ALLOCATION_ALIGNMENT + sizeof(Bytemap),\n+        ALLOCATION_ALIGNMENT);\n+    heap->bytemap = bytemap;\n+\n+    // Init heap for small objects\n+    heap->heapSize = minHeapSize;\n+    heap->heapStart = heapStart;\n+    heap->heapEnd = heapStart + minHeapSize / WORD_SIZE;\n+    heap->sweep.cursor = initialBlockCount;\n+    heap->lazySweep.cursorDone = initialBlockCount;\n+    heap->sweep.limit = initialBlockCount;\n+    heap->sweep.coalesceDone = initialBlockCount;\n+    heap->sweep.postSweepDone = true;\n+    Bytemap_Init(bytemap, heapStart, maxHeapSize);\n+    Allocator_Init(&allocator, &blockAllocator, bytemap, blockMetaStart,\n+                   heapStart);\n+\n+    LargeAllocator_Init(&largeAllocator, &blockAllocator, bytemap,\n+                        blockMetaStart, heapStart);\n+\n+    // Init all GCThreads\n+    sem_init(&heap->gcThreads.start, 0, 0);\n+    sem_init(&heap->gcThreads.start0, 0, 0);\n+\n+    // Init stats if enabled.\n+    // This must done before initializing other threads.\n+#ifdef ENABLE_GC_STATS\n+    char *statsFile = Settings_StatsFileName();\n+    if (statsFile != NULL) {\n+        heap->stats = malloc(sizeof(Stats));\n+        Stats_Init(heap->stats, statsFile, MUTATOR_THREAD_ID);\n+    } else {\n+#endif\n+        heap->stats = NULL;\n+#ifdef ENABLE_GC_STATS\n+    }\n+#endif\n+\n+\n+    int gcThreadCount = Settings_GCThreadCount();\n+    heap->gcThreads.count = gcThreadCount;\n+    heap->gcThreads.phase = gc_idle;\n+    GCThread *gcThreads = (GCThread *)malloc(sizeof(GCThread) * gcThreadCount);\n+    heap->gcThreads.all = (void *)gcThreads;\n+    for (int i = 0; i < gcThreadCount; i++) {\n+        Stats *stats;\n+#ifdef ENABLE_GC_STATS\n+        if (statsFile != NULL) {\n+            int len = strlen(statsFile) + 5;\n+            char *threadSpecificFile = (char *) malloc(len);\n+            snprintf(threadSpecificFile, len, \"%s.t%d\", statsFile, i);\n+            stats = malloc(sizeof(Stats));\n+            Stats_Init(stats, threadSpecificFile, (uint8_t)i);\n+        } else {\n+#endif\n+            stats = NULL;\n+#ifdef ENABLE_GC_STATS\n+        }\n+#endif\n+        GCThread_Init(&gcThreads[i], i, heap, stats);\n+    }\n+\n+    heap->mark.lastEnd_ns = scalanative_nano_time();\n+\n+    pthread_mutex_init(&heap->sweep.growMutex, NULL);\n+}\n+\n+word_t *Heap_AllocLarge(Heap *heap, uint32_t size) {\n+\n+    assert(size % ALLOCATION_ALIGNMENT == 0);\n+    assert(size >= MIN_BLOCK_SIZE);\n+\n+    Object *object = LargeAllocator_GetBlock(&largeAllocator, size);\n+    if (object != NULL) {\n+done:\n+        assert(object != NULL);\n+        assert(Heap_IsWordInHeap(heap, (word_t *)object));\n+        return (word_t *)object;\n+}\n+\n+    if (!Sweeper_IsSweepDone(heap)) {\n+        object = Sweeper_LazySweepLarge(heap, size);\n+        if (object != NULL)\n+            goto done;\n+    }\n+\n+    Heap_Collect(heap);\n+\n+    object = LargeAllocator_GetBlock(&largeAllocator, size);\n+    if (object != NULL)\n+        goto done;\n+\n+    if (!Sweeper_IsSweepDone(heap)) {\n+        object = Sweeper_LazySweepLarge(heap, size);\n+        if (object != NULL)\n+            goto done;\n+    }\n+\n+    size_t increment = MathUtils_DivAndRoundUp(size, BLOCK_TOTAL_SIZE);\n+    uint32_t pow2increment = 1U << MathUtils_Log2Ceil(increment);\n+    Heap_Grow(heap, pow2increment);\n+\n+    object = LargeAllocator_GetBlock(&largeAllocator, size);\n+\n+    goto done;\n+}\n+\n+NOINLINE word_t *Heap_allocSmallSlow(Heap *heap, uint32_t size) {\n+    Object *object = (Object *) Allocator_Alloc(&allocator, size);\n+\n+    if (object != NULL) {\n+done:\n+        assert(Heap_IsWordInHeap(heap, (word_t *)object));\n+        assert(object != NULL);\n+        ObjectMeta *objectMeta = Bytemap_Get(allocator.bytemap, (word_t *)object);\n+#ifdef DEBUG_ASSERT\n+        ObjectMeta_AssertIsValidAllocation(objectMeta, size);\n+#endif\n+        ObjectMeta_SetAllocated(objectMeta);\n+        return (word_t *)object;\n+    }\n+\n+    if (!Sweeper_IsSweepDone(heap)) {\n+        object = Sweeper_LazySweep(heap, size);"
  },
  {
    "id" : "ebb6a145-7c9a-4345-8675-cc07a1e1a830",
    "prId" : 1423,
    "comments" : [
      {
        "id" : "fdceb98f-35f5-47c9-94f8-197f7b31b613",
        "parentId" : null,
        "author" : {
          "login" : "densh",
          "name" : null,
          "avatarUrl" : "https://avatars1.githubusercontent.com/u/320966?u=784f6f761f35b8b7f3f787172b468334d6524524&v=4"
        },
        "body" : "The two lines above look wrong. `Heap_Collect` will grow heap appropriately after collection so that 50% of the heap is free blocks. We have a single mutator, so there is no chance of someone else allocating all of the 50% of the heap before we reach here. \r\n\r\nIn the hypothetical case of multiple mutator threads, it's better to do another collection instead of ad-hoc growth here. Because it looks like it might cause unbounded growth to the heap limit if this path is actually taken frequently. ",
        "createdAt" : "2019-02-01T10:09:51Z",
        "updatedAt" : "2019-07-17T09:53:34Z",
        "lastEditedBy" : {
          "login" : "densh",
          "name" : null,
          "avatarUrl" : "https://avatars1.githubusercontent.com/u/320966?u=784f6f761f35b8b7f3f787172b468334d6524524&v=4"
        },
        "tags" : [
          {
            "value" : "outdated"
          }
        ]
      },
      {
        "id" : "16992169-c505-439c-ad0d-4a4257ee311d",
        "parentId" : "fdceb98f-35f5-47c9-94f8-197f7b31b613",
        "author" : {
          "login" : "valdisxp1",
          "name" : null,
          "avatarUrl" : "https://avatars3.githubusercontent.com/u/2277076?v=4"
        },
        "body" : "This still could happen if the reserved space (`SWEEP_RESERVE_BLOCKS`) is exactly as large as the free space after heap growth. When sweep is done, GC master thread takes the reserved blocks out of the free blocks. There is a race condition between this and the mutator allocating. If the GC thread wins and there are no recycled blocks, then the small allocation must grow the heap.",
        "createdAt" : "2019-02-02T11:01:43Z",
        "updatedAt" : "2019-07-17T09:53:34Z",
        "lastEditedBy" : {
          "login" : "valdisxp1",
          "name" : null,
          "avatarUrl" : "https://avatars3.githubusercontent.com/u/2277076?v=4"
        },
        "tags" : [
          {
            "value" : "outdated"
          }
        ]
      }
    ],
    "commit" : "9349177c2456e08c91598c5a158c63a30876a4e0",
    "line" : null,
    "diffHunk" : "@@ -0,0 +1,542 @@\n+#include <stdlib.h>\n+#include <sys/mman.h>\n+#include <stdio.h>\n+#include \"Heap.h\"\n+#include \"Log.h\"\n+#include \"Allocator.h\"\n+#include \"Marker.h\"\n+#include \"State.h\"\n+#include \"utils/MathUtils.h\"\n+#include \"StackTrace.h\"\n+#include \"Settings.h\"\n+#include \"Memory.h\"\n+#include \"GCThread.h\"\n+#include \"Sweeper.h\"\n+#include <memory.h>\n+#include <time.h>\n+#include <inttypes.h>\n+#include <sched.h>\n+\n+// Allow read and write\n+#define HEAP_MEM_PROT (PROT_READ | PROT_WRITE)\n+// Map private anonymous memory, and prevent from reserving swap\n+#define HEAP_MEM_FLAGS (MAP_NORESERVE | MAP_PRIVATE | MAP_ANONYMOUS)\n+// Map anonymous memory (not a file)\n+#define HEAP_MEM_FD -1\n+#define HEAP_MEM_FD_OFFSET 0\n+\n+void Heap_exitWithOutOfMemory() {\n+    printf(\"Out of heap space\\n\");\n+    StackTrace_PrintStackTrace();\n+    exit(1);\n+}\n+\n+bool Heap_isGrowingPossible(Heap *heap, uint32_t incrementInBlocks) {\n+    return heap->blockCount + incrementInBlocks <= heap->maxBlockCount;\n+}\n+\n+size_t Heap_getMemoryLimit() {\n+    size_t memorySize = getMemorySize();\n+    if ((uint64_t)memorySize > MAX_HEAP_SIZE) {\n+        return (size_t)MAX_HEAP_SIZE;\n+    } else {\n+        return memorySize;\n+    }\n+}\n+\n+/**\n+ * Maps `MAX_SIZE` of memory and returns the first address aligned on\n+ * `alignement` mask\n+ */\n+word_t *Heap_mapAndAlign(size_t memoryLimit, size_t alignmentSize) {\n+    assert(alignmentSize % WORD_SIZE == 0);\n+    word_t *heapStart = mmap(NULL, memoryLimit, HEAP_MEM_PROT, HEAP_MEM_FLAGS,\n+                             HEAP_MEM_FD, HEAP_MEM_FD_OFFSET);\n+\n+    size_t alignmentMask = ~(alignmentSize - 1);\n+    // Heap start not aligned on\n+    if (((word_t)heapStart & alignmentMask) != (word_t)heapStart) {\n+        word_t *previousBlock = (word_t *)((word_t)heapStart & alignmentMask);\n+        heapStart = previousBlock + alignmentSize / WORD_SIZE;\n+    }\n+    return heapStart;\n+}\n+\n+/**\n+ * Allocates the heap struct and initializes it\n+ */\n+void Heap_Init(Heap *heap, size_t minHeapSize, size_t maxHeapSize) {\n+    size_t memoryLimit = Heap_getMemoryLimit();\n+\n+    if (maxHeapSize < MIN_HEAP_SIZE) {\n+        fprintf(stderr,\n+                \"SCALANATIVE_MAX_HEAP_SIZE too small to initialize heap.\\n\");\n+        fprintf(stderr, \"Minimum required: %zum \\n\",\n+                MIN_HEAP_SIZE / 1024 / 1024);\n+        fflush(stderr);\n+        exit(1);\n+    }\n+\n+    if (minHeapSize > memoryLimit) {\n+        fprintf(stderr, \"SCALANATIVE_MIN_HEAP_SIZE is too large.\\n\");\n+        fprintf(stderr, \"Maximum possible: %zug \\n\",\n+                memoryLimit / 1024 / 1024 / 1024);\n+        fflush(stderr);\n+        exit(1);\n+    }\n+\n+    if (maxHeapSize < minHeapSize) {\n+        fprintf(stderr, \"SCALANATIVE_MAX_HEAP_SIZE should be at least \"\n+                        \"SCALANATIVE_MIN_HEAP_SIZE\\n\");\n+        fflush(stderr);\n+        exit(1);\n+    }\n+\n+    if (minHeapSize < MIN_HEAP_SIZE) {\n+        minHeapSize = MIN_HEAP_SIZE;\n+    }\n+\n+    if (maxHeapSize == UNLIMITED_HEAP_SIZE) {\n+        maxHeapSize = memoryLimit;\n+    }\n+\n+    uint32_t maxNumberOfBlocks = maxHeapSize / SPACE_USED_PER_BLOCK;\n+    uint32_t initialBlockCount = minHeapSize / SPACE_USED_PER_BLOCK;\n+    heap->maxHeapSize = maxHeapSize;\n+    heap->blockCount = initialBlockCount;\n+    heap->maxBlockCount = maxNumberOfBlocks;\n+\n+    // reserve space for block headers\n+    size_t blockMetaSpaceSize = maxNumberOfBlocks * sizeof(BlockMeta);\n+    word_t *blockMetaStart = Heap_mapAndAlign(blockMetaSpaceSize, WORD_SIZE);\n+    heap->blockMetaStart = blockMetaStart;\n+    heap->blockMetaEnd =\n+        blockMetaStart + initialBlockCount * sizeof(BlockMeta) / WORD_SIZE;\n+\n+    // reserve space for line headers\n+    size_t lineMetaSpaceSize =\n+        (size_t)maxNumberOfBlocks * LINE_COUNT * LINE_METADATA_SIZE;\n+    word_t *lineMetaStart = Heap_mapAndAlign(lineMetaSpaceSize, WORD_SIZE);\n+    heap->lineMetaStart = lineMetaStart;\n+    assert(LINE_COUNT * LINE_SIZE == BLOCK_TOTAL_SIZE);\n+    assert(LINE_COUNT * LINE_METADATA_SIZE % WORD_SIZE == 0);\n+    heap->lineMetaEnd = lineMetaStart + initialBlockCount * LINE_COUNT *\n+                                            LINE_METADATA_SIZE / WORD_SIZE;\n+\n+    word_t *heapStart = Heap_mapAndAlign(maxHeapSize, BLOCK_TOTAL_SIZE);\n+\n+    BlockAllocator_Init(&blockAllocator, blockMetaStart, initialBlockCount);\n+    GreyList_Init(&heap->mark.empty);\n+    GreyList_Init(&heap->mark.full);\n+    uint32_t greyPacketCount = (uint32_t)(maxHeapSize * GREY_PACKET_RATIO / GREY_PACKET_SIZE);\n+    heap->mark.total = greyPacketCount;\n+    word_t* greyPacketsStart = Heap_mapAndAlign(greyPacketCount * sizeof(GreyPacket), WORD_SIZE);\n+    heap->greyPacketsStart = greyPacketsStart;\n+    GreyList_PushAll(&heap->mark.empty, greyPacketsStart, (GreyPacket *) greyPacketsStart, greyPacketCount);\n+\n+    // reserve space for bytemap\n+    Bytemap *bytemap = (Bytemap *)Heap_mapAndAlign(\n+        maxHeapSize / ALLOCATION_ALIGNMENT + sizeof(Bytemap),\n+        ALLOCATION_ALIGNMENT);\n+    heap->bytemap = bytemap;\n+\n+    // Init heap for small objects\n+    heap->heapSize = minHeapSize;\n+    heap->heapStart = heapStart;\n+    heap->heapEnd = heapStart + minHeapSize / WORD_SIZE;\n+    heap->sweep.cursor = initialBlockCount;\n+    heap->lazySweep.cursorDone = initialBlockCount;\n+    heap->sweep.limit = initialBlockCount;\n+    heap->sweep.coalesceDone = initialBlockCount;\n+    heap->sweep.postSweepDone = true;\n+    Bytemap_Init(bytemap, heapStart, maxHeapSize);\n+    Allocator_Init(&allocator, &blockAllocator, bytemap, blockMetaStart,\n+                   heapStart);\n+\n+    LargeAllocator_Init(&largeAllocator, &blockAllocator, bytemap,\n+                        blockMetaStart, heapStart);\n+\n+    // Init all GCThreads\n+    sem_init(&heap->gcThreads.start, 0, 0);\n+    sem_init(&heap->gcThreads.start0, 0, 0);\n+\n+    // Init stats if enabled.\n+    // This must done before initializing other threads.\n+#ifdef ENABLE_GC_STATS\n+    char *statsFile = Settings_StatsFileName();\n+    if (statsFile != NULL) {\n+        heap->stats = malloc(sizeof(Stats));\n+        Stats_Init(heap->stats, statsFile, MUTATOR_THREAD_ID);\n+    } else {\n+#endif\n+        heap->stats = NULL;\n+#ifdef ENABLE_GC_STATS\n+    }\n+#endif\n+\n+\n+    int gcThreadCount = Settings_GCThreadCount();\n+    heap->gcThreads.count = gcThreadCount;\n+    heap->gcThreads.phase = gc_idle;\n+    GCThread *gcThreads = (GCThread *)malloc(sizeof(GCThread) * gcThreadCount);\n+    heap->gcThreads.all = (void *)gcThreads;\n+    for (int i = 0; i < gcThreadCount; i++) {\n+        Stats *stats;\n+#ifdef ENABLE_GC_STATS\n+        if (statsFile != NULL) {\n+            int len = strlen(statsFile) + 5;\n+            char *threadSpecificFile = (char *) malloc(len);\n+            snprintf(threadSpecificFile, len, \"%s.t%d\", statsFile, i);\n+            stats = malloc(sizeof(Stats));\n+            Stats_Init(stats, threadSpecificFile, (uint8_t)i);\n+        } else {\n+#endif\n+            stats = NULL;\n+#ifdef ENABLE_GC_STATS\n+        }\n+#endif\n+        GCThread_Init(&gcThreads[i], i, heap, stats);\n+    }\n+\n+    heap->mark.lastEnd_ns = scalanative_nano_time();\n+\n+    pthread_mutex_init(&heap->sweep.growMutex, NULL);\n+}\n+\n+word_t *Heap_AllocLarge(Heap *heap, uint32_t size) {\n+\n+    assert(size % ALLOCATION_ALIGNMENT == 0);\n+    assert(size >= MIN_BLOCK_SIZE);\n+\n+    Object *object = LargeAllocator_GetBlock(&largeAllocator, size);\n+    if (object != NULL) {\n+done:\n+        assert(object != NULL);\n+        assert(Heap_IsWordInHeap(heap, (word_t *)object));\n+        return (word_t *)object;\n+}\n+\n+    if (!Sweeper_IsSweepDone(heap)) {\n+        object = Sweeper_LazySweepLarge(heap, size);\n+        if (object != NULL)\n+            goto done;\n+    }\n+\n+    Heap_Collect(heap);\n+\n+    object = LargeAllocator_GetBlock(&largeAllocator, size);\n+    if (object != NULL)\n+        goto done;\n+\n+    if (!Sweeper_IsSweepDone(heap)) {\n+        object = Sweeper_LazySweepLarge(heap, size);\n+        if (object != NULL)\n+            goto done;\n+    }\n+\n+    size_t increment = MathUtils_DivAndRoundUp(size, BLOCK_TOTAL_SIZE);\n+    uint32_t pow2increment = 1U << MathUtils_Log2Ceil(increment);\n+    Heap_Grow(heap, pow2increment);\n+\n+    object = LargeAllocator_GetBlock(&largeAllocator, size);\n+\n+    goto done;\n+}\n+\n+NOINLINE word_t *Heap_allocSmallSlow(Heap *heap, uint32_t size) {\n+    Object *object = (Object *) Allocator_Alloc(&allocator, size);\n+\n+    if (object != NULL) {\n+done:\n+        assert(Heap_IsWordInHeap(heap, (word_t *)object));\n+        assert(object != NULL);\n+        ObjectMeta *objectMeta = Bytemap_Get(allocator.bytemap, (word_t *)object);\n+#ifdef DEBUG_ASSERT\n+        ObjectMeta_AssertIsValidAllocation(objectMeta, size);\n+#endif\n+        ObjectMeta_SetAllocated(objectMeta);\n+        return (word_t *)object;\n+    }\n+\n+    if (!Sweeper_IsSweepDone(heap)) {\n+        object = Sweeper_LazySweep(heap, size);\n+\n+        if (object != NULL)\n+            goto done;\n+    }\n+\n+    Heap_Collect(heap);\n+    object = (Object *) Allocator_Alloc(&allocator, size);\n+\n+    if (object != NULL)\n+        goto done;\n+\n+    if (!Sweeper_IsSweepDone(heap)) {\n+        object = Sweeper_LazySweep(heap, size);\n+\n+        if (object != NULL)\n+            goto done;\n+    }\n+\n+    // A small object can always fit in a single free block\n+    // because it is no larger than 8K while the block is 32K.\n+    Heap_Grow(heap, 1);\n+    object = (Object *) Allocator_Alloc(&allocator, size);"
  },
  {
    "id" : "87df55d5-0321-40b8-8fcc-72d26021a10f",
    "prId" : 1423,
    "comments" : [
      {
        "id" : "1d032820-8f90-4ea6-a6d8-b600ebb5bb84",
        "parentId" : null,
        "author" : {
          "login" : "densh",
          "name" : null,
          "avatarUrl" : "https://avatars1.githubusercontent.com/u/320966?u=784f6f761f35b8b7f3f787172b468334d6524524&v=4"
        },
        "body" : "This needs to be factored out into a helper function `startMainGCThread` instead of a comment.",
        "createdAt" : "2019-02-01T10:11:03Z",
        "updatedAt" : "2019-07-17T09:53:34Z",
        "lastEditedBy" : {
          "login" : "densh",
          "name" : null,
          "avatarUrl" : "https://avatars1.githubusercontent.com/u/320966?u=784f6f761f35b8b7f3f787172b468334d6524524&v=4"
        },
        "tags" : [
          {
            "value" : "outdated"
          }
        ]
      }
    ],
    "commit" : "9349177c2456e08c91598c5a158c63a30876a4e0",
    "line" : null,
    "diffHunk" : "@@ -0,0 +1,542 @@\n+#include <stdlib.h>\n+#include <sys/mman.h>\n+#include <stdio.h>\n+#include \"Heap.h\"\n+#include \"Log.h\"\n+#include \"Allocator.h\"\n+#include \"Marker.h\"\n+#include \"State.h\"\n+#include \"utils/MathUtils.h\"\n+#include \"StackTrace.h\"\n+#include \"Settings.h\"\n+#include \"Memory.h\"\n+#include \"GCThread.h\"\n+#include \"Sweeper.h\"\n+#include <memory.h>\n+#include <time.h>\n+#include <inttypes.h>\n+#include <sched.h>\n+\n+// Allow read and write\n+#define HEAP_MEM_PROT (PROT_READ | PROT_WRITE)\n+// Map private anonymous memory, and prevent from reserving swap\n+#define HEAP_MEM_FLAGS (MAP_NORESERVE | MAP_PRIVATE | MAP_ANONYMOUS)\n+// Map anonymous memory (not a file)\n+#define HEAP_MEM_FD -1\n+#define HEAP_MEM_FD_OFFSET 0\n+\n+void Heap_exitWithOutOfMemory() {\n+    printf(\"Out of heap space\\n\");\n+    StackTrace_PrintStackTrace();\n+    exit(1);\n+}\n+\n+bool Heap_isGrowingPossible(Heap *heap, uint32_t incrementInBlocks) {\n+    return heap->blockCount + incrementInBlocks <= heap->maxBlockCount;\n+}\n+\n+size_t Heap_getMemoryLimit() {\n+    size_t memorySize = getMemorySize();\n+    if ((uint64_t)memorySize > MAX_HEAP_SIZE) {\n+        return (size_t)MAX_HEAP_SIZE;\n+    } else {\n+        return memorySize;\n+    }\n+}\n+\n+/**\n+ * Maps `MAX_SIZE` of memory and returns the first address aligned on\n+ * `alignement` mask\n+ */\n+word_t *Heap_mapAndAlign(size_t memoryLimit, size_t alignmentSize) {\n+    assert(alignmentSize % WORD_SIZE == 0);\n+    word_t *heapStart = mmap(NULL, memoryLimit, HEAP_MEM_PROT, HEAP_MEM_FLAGS,\n+                             HEAP_MEM_FD, HEAP_MEM_FD_OFFSET);\n+\n+    size_t alignmentMask = ~(alignmentSize - 1);\n+    // Heap start not aligned on\n+    if (((word_t)heapStart & alignmentMask) != (word_t)heapStart) {\n+        word_t *previousBlock = (word_t *)((word_t)heapStart & alignmentMask);\n+        heapStart = previousBlock + alignmentSize / WORD_SIZE;\n+    }\n+    return heapStart;\n+}\n+\n+/**\n+ * Allocates the heap struct and initializes it\n+ */\n+void Heap_Init(Heap *heap, size_t minHeapSize, size_t maxHeapSize) {\n+    size_t memoryLimit = Heap_getMemoryLimit();\n+\n+    if (maxHeapSize < MIN_HEAP_SIZE) {\n+        fprintf(stderr,\n+                \"SCALANATIVE_MAX_HEAP_SIZE too small to initialize heap.\\n\");\n+        fprintf(stderr, \"Minimum required: %zum \\n\",\n+                MIN_HEAP_SIZE / 1024 / 1024);\n+        fflush(stderr);\n+        exit(1);\n+    }\n+\n+    if (minHeapSize > memoryLimit) {\n+        fprintf(stderr, \"SCALANATIVE_MIN_HEAP_SIZE is too large.\\n\");\n+        fprintf(stderr, \"Maximum possible: %zug \\n\",\n+                memoryLimit / 1024 / 1024 / 1024);\n+        fflush(stderr);\n+        exit(1);\n+    }\n+\n+    if (maxHeapSize < minHeapSize) {\n+        fprintf(stderr, \"SCALANATIVE_MAX_HEAP_SIZE should be at least \"\n+                        \"SCALANATIVE_MIN_HEAP_SIZE\\n\");\n+        fflush(stderr);\n+        exit(1);\n+    }\n+\n+    if (minHeapSize < MIN_HEAP_SIZE) {\n+        minHeapSize = MIN_HEAP_SIZE;\n+    }\n+\n+    if (maxHeapSize == UNLIMITED_HEAP_SIZE) {\n+        maxHeapSize = memoryLimit;\n+    }\n+\n+    uint32_t maxNumberOfBlocks = maxHeapSize / SPACE_USED_PER_BLOCK;\n+    uint32_t initialBlockCount = minHeapSize / SPACE_USED_PER_BLOCK;\n+    heap->maxHeapSize = maxHeapSize;\n+    heap->blockCount = initialBlockCount;\n+    heap->maxBlockCount = maxNumberOfBlocks;\n+\n+    // reserve space for block headers\n+    size_t blockMetaSpaceSize = maxNumberOfBlocks * sizeof(BlockMeta);\n+    word_t *blockMetaStart = Heap_mapAndAlign(blockMetaSpaceSize, WORD_SIZE);\n+    heap->blockMetaStart = blockMetaStart;\n+    heap->blockMetaEnd =\n+        blockMetaStart + initialBlockCount * sizeof(BlockMeta) / WORD_SIZE;\n+\n+    // reserve space for line headers\n+    size_t lineMetaSpaceSize =\n+        (size_t)maxNumberOfBlocks * LINE_COUNT * LINE_METADATA_SIZE;\n+    word_t *lineMetaStart = Heap_mapAndAlign(lineMetaSpaceSize, WORD_SIZE);\n+    heap->lineMetaStart = lineMetaStart;\n+    assert(LINE_COUNT * LINE_SIZE == BLOCK_TOTAL_SIZE);\n+    assert(LINE_COUNT * LINE_METADATA_SIZE % WORD_SIZE == 0);\n+    heap->lineMetaEnd = lineMetaStart + initialBlockCount * LINE_COUNT *\n+                                            LINE_METADATA_SIZE / WORD_SIZE;\n+\n+    word_t *heapStart = Heap_mapAndAlign(maxHeapSize, BLOCK_TOTAL_SIZE);\n+\n+    BlockAllocator_Init(&blockAllocator, blockMetaStart, initialBlockCount);\n+    GreyList_Init(&heap->mark.empty);\n+    GreyList_Init(&heap->mark.full);\n+    uint32_t greyPacketCount = (uint32_t)(maxHeapSize * GREY_PACKET_RATIO / GREY_PACKET_SIZE);\n+    heap->mark.total = greyPacketCount;\n+    word_t* greyPacketsStart = Heap_mapAndAlign(greyPacketCount * sizeof(GreyPacket), WORD_SIZE);\n+    heap->greyPacketsStart = greyPacketsStart;\n+    GreyList_PushAll(&heap->mark.empty, greyPacketsStart, (GreyPacket *) greyPacketsStart, greyPacketCount);\n+\n+    // reserve space for bytemap\n+    Bytemap *bytemap = (Bytemap *)Heap_mapAndAlign(\n+        maxHeapSize / ALLOCATION_ALIGNMENT + sizeof(Bytemap),\n+        ALLOCATION_ALIGNMENT);\n+    heap->bytemap = bytemap;\n+\n+    // Init heap for small objects\n+    heap->heapSize = minHeapSize;\n+    heap->heapStart = heapStart;\n+    heap->heapEnd = heapStart + minHeapSize / WORD_SIZE;\n+    heap->sweep.cursor = initialBlockCount;\n+    heap->lazySweep.cursorDone = initialBlockCount;\n+    heap->sweep.limit = initialBlockCount;\n+    heap->sweep.coalesceDone = initialBlockCount;\n+    heap->sweep.postSweepDone = true;\n+    Bytemap_Init(bytemap, heapStart, maxHeapSize);\n+    Allocator_Init(&allocator, &blockAllocator, bytemap, blockMetaStart,\n+                   heapStart);\n+\n+    LargeAllocator_Init(&largeAllocator, &blockAllocator, bytemap,\n+                        blockMetaStart, heapStart);\n+\n+    // Init all GCThreads\n+    sem_init(&heap->gcThreads.start, 0, 0);\n+    sem_init(&heap->gcThreads.start0, 0, 0);\n+\n+    // Init stats if enabled.\n+    // This must done before initializing other threads.\n+#ifdef ENABLE_GC_STATS\n+    char *statsFile = Settings_StatsFileName();\n+    if (statsFile != NULL) {\n+        heap->stats = malloc(sizeof(Stats));\n+        Stats_Init(heap->stats, statsFile, MUTATOR_THREAD_ID);\n+    } else {\n+#endif\n+        heap->stats = NULL;\n+#ifdef ENABLE_GC_STATS\n+    }\n+#endif\n+\n+\n+    int gcThreadCount = Settings_GCThreadCount();\n+    heap->gcThreads.count = gcThreadCount;\n+    heap->gcThreads.phase = gc_idle;\n+    GCThread *gcThreads = (GCThread *)malloc(sizeof(GCThread) * gcThreadCount);\n+    heap->gcThreads.all = (void *)gcThreads;\n+    for (int i = 0; i < gcThreadCount; i++) {\n+        Stats *stats;\n+#ifdef ENABLE_GC_STATS\n+        if (statsFile != NULL) {\n+            int len = strlen(statsFile) + 5;\n+            char *threadSpecificFile = (char *) malloc(len);\n+            snprintf(threadSpecificFile, len, \"%s.t%d\", statsFile, i);\n+            stats = malloc(sizeof(Stats));\n+            Stats_Init(stats, threadSpecificFile, (uint8_t)i);\n+        } else {\n+#endif\n+            stats = NULL;\n+#ifdef ENABLE_GC_STATS\n+        }\n+#endif\n+        GCThread_Init(&gcThreads[i], i, heap, stats);\n+    }\n+\n+    heap->mark.lastEnd_ns = scalanative_nano_time();\n+\n+    pthread_mutex_init(&heap->sweep.growMutex, NULL);\n+}\n+\n+word_t *Heap_AllocLarge(Heap *heap, uint32_t size) {\n+\n+    assert(size % ALLOCATION_ALIGNMENT == 0);\n+    assert(size >= MIN_BLOCK_SIZE);\n+\n+    Object *object = LargeAllocator_GetBlock(&largeAllocator, size);\n+    if (object != NULL) {\n+done:\n+        assert(object != NULL);\n+        assert(Heap_IsWordInHeap(heap, (word_t *)object));\n+        return (word_t *)object;\n+}\n+\n+    if (!Sweeper_IsSweepDone(heap)) {\n+        object = Sweeper_LazySweepLarge(heap, size);\n+        if (object != NULL)\n+            goto done;\n+    }\n+\n+    Heap_Collect(heap);\n+\n+    object = LargeAllocator_GetBlock(&largeAllocator, size);\n+    if (object != NULL)\n+        goto done;\n+\n+    if (!Sweeper_IsSweepDone(heap)) {\n+        object = Sweeper_LazySweepLarge(heap, size);\n+        if (object != NULL)\n+            goto done;\n+    }\n+\n+    size_t increment = MathUtils_DivAndRoundUp(size, BLOCK_TOTAL_SIZE);\n+    uint32_t pow2increment = 1U << MathUtils_Log2Ceil(increment);\n+    Heap_Grow(heap, pow2increment);\n+\n+    object = LargeAllocator_GetBlock(&largeAllocator, size);\n+\n+    goto done;\n+}\n+\n+NOINLINE word_t *Heap_allocSmallSlow(Heap *heap, uint32_t size) {\n+    Object *object = (Object *) Allocator_Alloc(&allocator, size);\n+\n+    if (object != NULL) {\n+done:\n+        assert(Heap_IsWordInHeap(heap, (word_t *)object));\n+        assert(object != NULL);\n+        ObjectMeta *objectMeta = Bytemap_Get(allocator.bytemap, (word_t *)object);\n+#ifdef DEBUG_ASSERT\n+        ObjectMeta_AssertIsValidAllocation(objectMeta, size);\n+#endif\n+        ObjectMeta_SetAllocated(objectMeta);\n+        return (word_t *)object;\n+    }\n+\n+    if (!Sweeper_IsSweepDone(heap)) {\n+        object = Sweeper_LazySweep(heap, size);\n+\n+        if (object != NULL)\n+            goto done;\n+    }\n+\n+    Heap_Collect(heap);\n+    object = (Object *) Allocator_Alloc(&allocator, size);\n+\n+    if (object != NULL)\n+        goto done;\n+\n+    if (!Sweeper_IsSweepDone(heap)) {\n+        object = Sweeper_LazySweep(heap, size);\n+\n+        if (object != NULL)\n+            goto done;\n+    }\n+\n+    // A small object can always fit in a single free block\n+    // because it is no larger than 8K while the block is 32K.\n+    Heap_Grow(heap, 1);\n+    object = (Object *) Allocator_Alloc(&allocator, size);\n+\n+    goto done;\n+}\n+\n+INLINE word_t *Heap_AllocSmall(Heap *heap, uint32_t size) {\n+    assert(size % ALLOCATION_ALIGNMENT == 0);\n+    assert(size < MIN_BLOCK_SIZE);\n+\n+    word_t *start = allocator.cursor;\n+    word_t *end = (word_t *)((uint8_t *)start + size);\n+\n+    // Checks if the end of the block overlaps with the limit\n+    if (end >= allocator.limit) {\n+        return Heap_allocSmallSlow(heap, size);\n+    }\n+\n+    allocator.cursor = end;\n+\n+    memset(start, 0, size);\n+\n+    Object *object = (Object *)start;\n+    ObjectMeta *objectMeta = Bytemap_Get(allocator.bytemap, (word_t *)object);\n+#ifdef DEBUG_ASSERT\n+    ObjectMeta_AssertIsValidAllocation(objectMeta, size);\n+#endif\n+    ObjectMeta_SetAllocated(objectMeta);\n+\n+    __builtin_prefetch(object + 36, 0, 3);\n+\n+    assert(Heap_IsWordInHeap(heap, (word_t *)object));\n+    return (word_t *)object;\n+}\n+\n+word_t *Heap_Alloc(Heap *heap, uint32_t objectSize) {\n+    assert(objectSize % ALLOCATION_ALIGNMENT == 0);\n+\n+    if (objectSize >= LARGE_BLOCK_SIZE) {\n+        return Heap_AllocLarge(heap, objectSize);\n+    } else {\n+        return Heap_AllocSmall(heap, objectSize);\n+    }\n+}\n+\n+#ifdef DEBUG_ASSERT\n+void Heap_clearIsSwept(Heap *heap) {\n+    BlockMeta *current = (BlockMeta *)heap->blockMetaStart;\n+    BlockMeta *limit = (BlockMeta *)heap->blockMetaEnd;\n+    while (current < limit) {\n+        BlockMeta *reserveFirst = (BlockMeta *) blockAllocator.reservedSuperblock;\n+        BlockMeta *reserveLimit = reserveFirst + SWEEP_RESERVE_BLOCKS;\n+        if (current < reserveFirst || current >= reserveLimit) {\n+            assert(reserveFirst !=  NULL);\n+            current->debugFlag = dbg_must_sweep;\n+        }\n+        current++;\n+    }\n+}\n+\n+void Heap_assertIsConsistent(Heap *heap) {\n+    BlockMeta *current = (BlockMeta *)heap->blockMetaStart;\n+    LineMeta *lineMetas = (LineMeta *)heap->lineMetaStart;\n+    BlockMeta *limit = (BlockMeta *)heap->blockMetaEnd;\n+    ObjectMeta *currentBlockStart = Bytemap_Get(heap->bytemap, heap->heapStart);\n+    while (current < limit) {\n+        assert(!BlockMeta_IsCoalesceMe(current));\n+        assert(!BlockMeta_IsSuperblockStartMe(current));\n+        assert(!BlockMeta_IsSuperblockTail(current));\n+        assert(!BlockMeta_IsMarked(current));\n+\n+        int size = 1;\n+        if (BlockMeta_IsSuperblockStart(current)) {\n+            size = BlockMeta_SuperblockSize(current);\n+        }\n+        BlockMeta *next = current + size;\n+        LineMeta *nextLineMetas = lineMetas + LINE_COUNT * size;\n+        ObjectMeta *nextBlockStart =\n+            currentBlockStart +\n+            (WORDS_IN_BLOCK / ALLOCATION_ALIGNMENT_WORDS) * size;\n+\n+        for (LineMeta *line = lineMetas; line < nextLineMetas; line++) {\n+            assert(!Line_IsMarked(line));\n+        }\n+        for (ObjectMeta *object = currentBlockStart; object < nextBlockStart;\n+             object++) {\n+            assert(!ObjectMeta_IsMarked(object));\n+        }\n+\n+        current = next;\n+        lineMetas = nextLineMetas;\n+        currentBlockStart = nextBlockStart;\n+    }\n+    assert(current == limit);\n+}\n+#endif\n+\n+void Heap_Collect(Heap *heap) {\n+#ifdef ENABLE_GC_STATS\n+    Stats *stats = heap->stats;\n+    if (stats != NULL) {\n+        stats->collection_start_ns = scalanative_nano_time();\n+    }\n+#else\n+    Stats *stats = NULL;\n+#endif\n+    assert(Sweeper_IsSweepDone(heap));\n+#ifdef DEBUG_ASSERT\n+    Heap_clearIsSwept(heap);\n+    Heap_assertIsConsistent(heap);\n+#endif\n+    heap->mark.lastEnd_ns = heap->mark.currentEnd_ns;\n+    heap->mark.currentStart_ns = scalanative_nano_time();\n+    Marker_MarkRoots(heap, stats);\n+    heap->gcThreads.phase = gc_mark;\n+    // make sure the gc phase is propagated\n+    atomic_thread_fence(memory_order_release);\n+    // start main gc thread (0)\n+    sem_post(&heap->gcThreads.start0);"
  },
  {
    "id" : "993a5f3b-8c67-4d57-a56d-8d5acaa96deb",
    "prId" : 1423,
    "comments" : [
      {
        "id" : "ef003ec0-bb42-4e50-a9b1-8648f46d3994",
        "parentId" : null,
        "author" : {
          "login" : "densh",
          "name" : null,
          "avatarUrl" : "https://avatars1.githubusercontent.com/u/320966?u=784f6f761f35b8b7f3f787172b468334d6524524&v=4"
        },
        "body" : "This should also be a helper function. It would be nice to isolate all of the phase/thread management into its own module with clear method names. ",
        "createdAt" : "2019-02-01T10:12:01Z",
        "updatedAt" : "2019-07-17T09:53:34Z",
        "lastEditedBy" : {
          "login" : "densh",
          "name" : null,
          "avatarUrl" : "https://avatars1.githubusercontent.com/u/320966?u=784f6f761f35b8b7f3f787172b468334d6524524&v=4"
        },
        "tags" : [
          {
            "value" : "outdated"
          }
        ]
      }
    ],
    "commit" : "9349177c2456e08c91598c5a158c63a30876a4e0",
    "line" : null,
    "diffHunk" : "@@ -0,0 +1,542 @@\n+#include <stdlib.h>\n+#include <sys/mman.h>\n+#include <stdio.h>\n+#include \"Heap.h\"\n+#include \"Log.h\"\n+#include \"Allocator.h\"\n+#include \"Marker.h\"\n+#include \"State.h\"\n+#include \"utils/MathUtils.h\"\n+#include \"StackTrace.h\"\n+#include \"Settings.h\"\n+#include \"Memory.h\"\n+#include \"GCThread.h\"\n+#include \"Sweeper.h\"\n+#include <memory.h>\n+#include <time.h>\n+#include <inttypes.h>\n+#include <sched.h>\n+\n+// Allow read and write\n+#define HEAP_MEM_PROT (PROT_READ | PROT_WRITE)\n+// Map private anonymous memory, and prevent from reserving swap\n+#define HEAP_MEM_FLAGS (MAP_NORESERVE | MAP_PRIVATE | MAP_ANONYMOUS)\n+// Map anonymous memory (not a file)\n+#define HEAP_MEM_FD -1\n+#define HEAP_MEM_FD_OFFSET 0\n+\n+void Heap_exitWithOutOfMemory() {\n+    printf(\"Out of heap space\\n\");\n+    StackTrace_PrintStackTrace();\n+    exit(1);\n+}\n+\n+bool Heap_isGrowingPossible(Heap *heap, uint32_t incrementInBlocks) {\n+    return heap->blockCount + incrementInBlocks <= heap->maxBlockCount;\n+}\n+\n+size_t Heap_getMemoryLimit() {\n+    size_t memorySize = getMemorySize();\n+    if ((uint64_t)memorySize > MAX_HEAP_SIZE) {\n+        return (size_t)MAX_HEAP_SIZE;\n+    } else {\n+        return memorySize;\n+    }\n+}\n+\n+/**\n+ * Maps `MAX_SIZE` of memory and returns the first address aligned on\n+ * `alignement` mask\n+ */\n+word_t *Heap_mapAndAlign(size_t memoryLimit, size_t alignmentSize) {\n+    assert(alignmentSize % WORD_SIZE == 0);\n+    word_t *heapStart = mmap(NULL, memoryLimit, HEAP_MEM_PROT, HEAP_MEM_FLAGS,\n+                             HEAP_MEM_FD, HEAP_MEM_FD_OFFSET);\n+\n+    size_t alignmentMask = ~(alignmentSize - 1);\n+    // Heap start not aligned on\n+    if (((word_t)heapStart & alignmentMask) != (word_t)heapStart) {\n+        word_t *previousBlock = (word_t *)((word_t)heapStart & alignmentMask);\n+        heapStart = previousBlock + alignmentSize / WORD_SIZE;\n+    }\n+    return heapStart;\n+}\n+\n+/**\n+ * Allocates the heap struct and initializes it\n+ */\n+void Heap_Init(Heap *heap, size_t minHeapSize, size_t maxHeapSize) {\n+    size_t memoryLimit = Heap_getMemoryLimit();\n+\n+    if (maxHeapSize < MIN_HEAP_SIZE) {\n+        fprintf(stderr,\n+                \"SCALANATIVE_MAX_HEAP_SIZE too small to initialize heap.\\n\");\n+        fprintf(stderr, \"Minimum required: %zum \\n\",\n+                MIN_HEAP_SIZE / 1024 / 1024);\n+        fflush(stderr);\n+        exit(1);\n+    }\n+\n+    if (minHeapSize > memoryLimit) {\n+        fprintf(stderr, \"SCALANATIVE_MIN_HEAP_SIZE is too large.\\n\");\n+        fprintf(stderr, \"Maximum possible: %zug \\n\",\n+                memoryLimit / 1024 / 1024 / 1024);\n+        fflush(stderr);\n+        exit(1);\n+    }\n+\n+    if (maxHeapSize < minHeapSize) {\n+        fprintf(stderr, \"SCALANATIVE_MAX_HEAP_SIZE should be at least \"\n+                        \"SCALANATIVE_MIN_HEAP_SIZE\\n\");\n+        fflush(stderr);\n+        exit(1);\n+    }\n+\n+    if (minHeapSize < MIN_HEAP_SIZE) {\n+        minHeapSize = MIN_HEAP_SIZE;\n+    }\n+\n+    if (maxHeapSize == UNLIMITED_HEAP_SIZE) {\n+        maxHeapSize = memoryLimit;\n+    }\n+\n+    uint32_t maxNumberOfBlocks = maxHeapSize / SPACE_USED_PER_BLOCK;\n+    uint32_t initialBlockCount = minHeapSize / SPACE_USED_PER_BLOCK;\n+    heap->maxHeapSize = maxHeapSize;\n+    heap->blockCount = initialBlockCount;\n+    heap->maxBlockCount = maxNumberOfBlocks;\n+\n+    // reserve space for block headers\n+    size_t blockMetaSpaceSize = maxNumberOfBlocks * sizeof(BlockMeta);\n+    word_t *blockMetaStart = Heap_mapAndAlign(blockMetaSpaceSize, WORD_SIZE);\n+    heap->blockMetaStart = blockMetaStart;\n+    heap->blockMetaEnd =\n+        blockMetaStart + initialBlockCount * sizeof(BlockMeta) / WORD_SIZE;\n+\n+    // reserve space for line headers\n+    size_t lineMetaSpaceSize =\n+        (size_t)maxNumberOfBlocks * LINE_COUNT * LINE_METADATA_SIZE;\n+    word_t *lineMetaStart = Heap_mapAndAlign(lineMetaSpaceSize, WORD_SIZE);\n+    heap->lineMetaStart = lineMetaStart;\n+    assert(LINE_COUNT * LINE_SIZE == BLOCK_TOTAL_SIZE);\n+    assert(LINE_COUNT * LINE_METADATA_SIZE % WORD_SIZE == 0);\n+    heap->lineMetaEnd = lineMetaStart + initialBlockCount * LINE_COUNT *\n+                                            LINE_METADATA_SIZE / WORD_SIZE;\n+\n+    word_t *heapStart = Heap_mapAndAlign(maxHeapSize, BLOCK_TOTAL_SIZE);\n+\n+    BlockAllocator_Init(&blockAllocator, blockMetaStart, initialBlockCount);\n+    GreyList_Init(&heap->mark.empty);\n+    GreyList_Init(&heap->mark.full);\n+    uint32_t greyPacketCount = (uint32_t)(maxHeapSize * GREY_PACKET_RATIO / GREY_PACKET_SIZE);\n+    heap->mark.total = greyPacketCount;\n+    word_t* greyPacketsStart = Heap_mapAndAlign(greyPacketCount * sizeof(GreyPacket), WORD_SIZE);\n+    heap->greyPacketsStart = greyPacketsStart;\n+    GreyList_PushAll(&heap->mark.empty, greyPacketsStart, (GreyPacket *) greyPacketsStart, greyPacketCount);\n+\n+    // reserve space for bytemap\n+    Bytemap *bytemap = (Bytemap *)Heap_mapAndAlign(\n+        maxHeapSize / ALLOCATION_ALIGNMENT + sizeof(Bytemap),\n+        ALLOCATION_ALIGNMENT);\n+    heap->bytemap = bytemap;\n+\n+    // Init heap for small objects\n+    heap->heapSize = minHeapSize;\n+    heap->heapStart = heapStart;\n+    heap->heapEnd = heapStart + minHeapSize / WORD_SIZE;\n+    heap->sweep.cursor = initialBlockCount;\n+    heap->lazySweep.cursorDone = initialBlockCount;\n+    heap->sweep.limit = initialBlockCount;\n+    heap->sweep.coalesceDone = initialBlockCount;\n+    heap->sweep.postSweepDone = true;\n+    Bytemap_Init(bytemap, heapStart, maxHeapSize);\n+    Allocator_Init(&allocator, &blockAllocator, bytemap, blockMetaStart,\n+                   heapStart);\n+\n+    LargeAllocator_Init(&largeAllocator, &blockAllocator, bytemap,\n+                        blockMetaStart, heapStart);\n+\n+    // Init all GCThreads\n+    sem_init(&heap->gcThreads.start, 0, 0);\n+    sem_init(&heap->gcThreads.start0, 0, 0);\n+\n+    // Init stats if enabled.\n+    // This must done before initializing other threads.\n+#ifdef ENABLE_GC_STATS\n+    char *statsFile = Settings_StatsFileName();\n+    if (statsFile != NULL) {\n+        heap->stats = malloc(sizeof(Stats));\n+        Stats_Init(heap->stats, statsFile, MUTATOR_THREAD_ID);\n+    } else {\n+#endif\n+        heap->stats = NULL;\n+#ifdef ENABLE_GC_STATS\n+    }\n+#endif\n+\n+\n+    int gcThreadCount = Settings_GCThreadCount();\n+    heap->gcThreads.count = gcThreadCount;\n+    heap->gcThreads.phase = gc_idle;\n+    GCThread *gcThreads = (GCThread *)malloc(sizeof(GCThread) * gcThreadCount);\n+    heap->gcThreads.all = (void *)gcThreads;\n+    for (int i = 0; i < gcThreadCount; i++) {\n+        Stats *stats;\n+#ifdef ENABLE_GC_STATS\n+        if (statsFile != NULL) {\n+            int len = strlen(statsFile) + 5;\n+            char *threadSpecificFile = (char *) malloc(len);\n+            snprintf(threadSpecificFile, len, \"%s.t%d\", statsFile, i);\n+            stats = malloc(sizeof(Stats));\n+            Stats_Init(stats, threadSpecificFile, (uint8_t)i);\n+        } else {\n+#endif\n+            stats = NULL;\n+#ifdef ENABLE_GC_STATS\n+        }\n+#endif\n+        GCThread_Init(&gcThreads[i], i, heap, stats);\n+    }\n+\n+    heap->mark.lastEnd_ns = scalanative_nano_time();\n+\n+    pthread_mutex_init(&heap->sweep.growMutex, NULL);\n+}\n+\n+word_t *Heap_AllocLarge(Heap *heap, uint32_t size) {\n+\n+    assert(size % ALLOCATION_ALIGNMENT == 0);\n+    assert(size >= MIN_BLOCK_SIZE);\n+\n+    Object *object = LargeAllocator_GetBlock(&largeAllocator, size);\n+    if (object != NULL) {\n+done:\n+        assert(object != NULL);\n+        assert(Heap_IsWordInHeap(heap, (word_t *)object));\n+        return (word_t *)object;\n+}\n+\n+    if (!Sweeper_IsSweepDone(heap)) {\n+        object = Sweeper_LazySweepLarge(heap, size);\n+        if (object != NULL)\n+            goto done;\n+    }\n+\n+    Heap_Collect(heap);\n+\n+    object = LargeAllocator_GetBlock(&largeAllocator, size);\n+    if (object != NULL)\n+        goto done;\n+\n+    if (!Sweeper_IsSweepDone(heap)) {\n+        object = Sweeper_LazySweepLarge(heap, size);\n+        if (object != NULL)\n+            goto done;\n+    }\n+\n+    size_t increment = MathUtils_DivAndRoundUp(size, BLOCK_TOTAL_SIZE);\n+    uint32_t pow2increment = 1U << MathUtils_Log2Ceil(increment);\n+    Heap_Grow(heap, pow2increment);\n+\n+    object = LargeAllocator_GetBlock(&largeAllocator, size);\n+\n+    goto done;\n+}\n+\n+NOINLINE word_t *Heap_allocSmallSlow(Heap *heap, uint32_t size) {\n+    Object *object = (Object *) Allocator_Alloc(&allocator, size);\n+\n+    if (object != NULL) {\n+done:\n+        assert(Heap_IsWordInHeap(heap, (word_t *)object));\n+        assert(object != NULL);\n+        ObjectMeta *objectMeta = Bytemap_Get(allocator.bytemap, (word_t *)object);\n+#ifdef DEBUG_ASSERT\n+        ObjectMeta_AssertIsValidAllocation(objectMeta, size);\n+#endif\n+        ObjectMeta_SetAllocated(objectMeta);\n+        return (word_t *)object;\n+    }\n+\n+    if (!Sweeper_IsSweepDone(heap)) {\n+        object = Sweeper_LazySweep(heap, size);\n+\n+        if (object != NULL)\n+            goto done;\n+    }\n+\n+    Heap_Collect(heap);\n+    object = (Object *) Allocator_Alloc(&allocator, size);\n+\n+    if (object != NULL)\n+        goto done;\n+\n+    if (!Sweeper_IsSweepDone(heap)) {\n+        object = Sweeper_LazySweep(heap, size);\n+\n+        if (object != NULL)\n+            goto done;\n+    }\n+\n+    // A small object can always fit in a single free block\n+    // because it is no larger than 8K while the block is 32K.\n+    Heap_Grow(heap, 1);\n+    object = (Object *) Allocator_Alloc(&allocator, size);\n+\n+    goto done;\n+}\n+\n+INLINE word_t *Heap_AllocSmall(Heap *heap, uint32_t size) {\n+    assert(size % ALLOCATION_ALIGNMENT == 0);\n+    assert(size < MIN_BLOCK_SIZE);\n+\n+    word_t *start = allocator.cursor;\n+    word_t *end = (word_t *)((uint8_t *)start + size);\n+\n+    // Checks if the end of the block overlaps with the limit\n+    if (end >= allocator.limit) {\n+        return Heap_allocSmallSlow(heap, size);\n+    }\n+\n+    allocator.cursor = end;\n+\n+    memset(start, 0, size);\n+\n+    Object *object = (Object *)start;\n+    ObjectMeta *objectMeta = Bytemap_Get(allocator.bytemap, (word_t *)object);\n+#ifdef DEBUG_ASSERT\n+    ObjectMeta_AssertIsValidAllocation(objectMeta, size);\n+#endif\n+    ObjectMeta_SetAllocated(objectMeta);\n+\n+    __builtin_prefetch(object + 36, 0, 3);\n+\n+    assert(Heap_IsWordInHeap(heap, (word_t *)object));\n+    return (word_t *)object;\n+}\n+\n+word_t *Heap_Alloc(Heap *heap, uint32_t objectSize) {\n+    assert(objectSize % ALLOCATION_ALIGNMENT == 0);\n+\n+    if (objectSize >= LARGE_BLOCK_SIZE) {\n+        return Heap_AllocLarge(heap, objectSize);\n+    } else {\n+        return Heap_AllocSmall(heap, objectSize);\n+    }\n+}\n+\n+#ifdef DEBUG_ASSERT\n+void Heap_clearIsSwept(Heap *heap) {\n+    BlockMeta *current = (BlockMeta *)heap->blockMetaStart;\n+    BlockMeta *limit = (BlockMeta *)heap->blockMetaEnd;\n+    while (current < limit) {\n+        BlockMeta *reserveFirst = (BlockMeta *) blockAllocator.reservedSuperblock;\n+        BlockMeta *reserveLimit = reserveFirst + SWEEP_RESERVE_BLOCKS;\n+        if (current < reserveFirst || current >= reserveLimit) {\n+            assert(reserveFirst !=  NULL);\n+            current->debugFlag = dbg_must_sweep;\n+        }\n+        current++;\n+    }\n+}\n+\n+void Heap_assertIsConsistent(Heap *heap) {\n+    BlockMeta *current = (BlockMeta *)heap->blockMetaStart;\n+    LineMeta *lineMetas = (LineMeta *)heap->lineMetaStart;\n+    BlockMeta *limit = (BlockMeta *)heap->blockMetaEnd;\n+    ObjectMeta *currentBlockStart = Bytemap_Get(heap->bytemap, heap->heapStart);\n+    while (current < limit) {\n+        assert(!BlockMeta_IsCoalesceMe(current));\n+        assert(!BlockMeta_IsSuperblockStartMe(current));\n+        assert(!BlockMeta_IsSuperblockTail(current));\n+        assert(!BlockMeta_IsMarked(current));\n+\n+        int size = 1;\n+        if (BlockMeta_IsSuperblockStart(current)) {\n+            size = BlockMeta_SuperblockSize(current);\n+        }\n+        BlockMeta *next = current + size;\n+        LineMeta *nextLineMetas = lineMetas + LINE_COUNT * size;\n+        ObjectMeta *nextBlockStart =\n+            currentBlockStart +\n+            (WORDS_IN_BLOCK / ALLOCATION_ALIGNMENT_WORDS) * size;\n+\n+        for (LineMeta *line = lineMetas; line < nextLineMetas; line++) {\n+            assert(!Line_IsMarked(line));\n+        }\n+        for (ObjectMeta *object = currentBlockStart; object < nextBlockStart;\n+             object++) {\n+            assert(!ObjectMeta_IsMarked(object));\n+        }\n+\n+        current = next;\n+        lineMetas = nextLineMetas;\n+        currentBlockStart = nextBlockStart;\n+    }\n+    assert(current == limit);\n+}\n+#endif\n+\n+void Heap_Collect(Heap *heap) {\n+#ifdef ENABLE_GC_STATS\n+    Stats *stats = heap->stats;\n+    if (stats != NULL) {\n+        stats->collection_start_ns = scalanative_nano_time();\n+    }\n+#else\n+    Stats *stats = NULL;\n+#endif\n+    assert(Sweeper_IsSweepDone(heap));\n+#ifdef DEBUG_ASSERT\n+    Heap_clearIsSwept(heap);\n+    Heap_assertIsConsistent(heap);\n+#endif\n+    heap->mark.lastEnd_ns = heap->mark.currentEnd_ns;\n+    heap->mark.currentStart_ns = scalanative_nano_time();\n+    Marker_MarkRoots(heap, stats);\n+    heap->gcThreads.phase = gc_mark;\n+    // make sure the gc phase is propagated\n+    atomic_thread_fence(memory_order_release);\n+    // start main gc thread (0)\n+    sem_post(&heap->gcThreads.start0);\n+    while (!Marker_IsMarkDone(heap)) {\n+        Marker_Mark(heap, stats);\n+        if (!Marker_IsMarkDone(heap)) {\n+            sched_yield();\n+        }\n+    }\n+    heap->gcThreads.phase = gc_idle;"
  },
  {
    "id" : "53b7533c-30a8-425a-9f6e-41f8fed58d60",
    "prId" : 1423,
    "comments" : [
      {
        "id" : "313600be-be2b-41e9-b734-97500bf4a1a7",
        "parentId" : null,
        "author" : {
          "login" : "densh",
          "name" : null,
          "avatarUrl" : "https://avatars1.githubusercontent.com/u/320966?u=784f6f761f35b8b7f3f787172b468334d6524524&v=4"
        },
        "body" : "Most of the code in `Heap_Collect` belongs to either the `Marker` or phase management module. The heap part of the code should be something along the lines of:\r\n1. Stats boilerplate\r\n2. Call phases to start marking phase.\r\n2. Call marker entry point method to do actual marking on the mutator thread until it's done.\r\n3. Call phases to trigger sweeping phase.\r\n3. Call sweeper entry point.",
        "createdAt" : "2019-02-01T10:15:28Z",
        "updatedAt" : "2019-07-17T09:53:34Z",
        "lastEditedBy" : {
          "login" : "densh",
          "name" : null,
          "avatarUrl" : "https://avatars1.githubusercontent.com/u/320966?u=784f6f761f35b8b7f3f787172b468334d6524524&v=4"
        },
        "tags" : [
        ]
      }
    ],
    "commit" : "9349177c2456e08c91598c5a158c63a30876a4e0",
    "line" : 226,
    "diffHunk" : "@@ -0,0 +1,542 @@\n+#include <stdlib.h>\n+#include <sys/mman.h>\n+#include <stdio.h>\n+#include \"Heap.h\"\n+#include \"Log.h\"\n+#include \"Allocator.h\"\n+#include \"Marker.h\"\n+#include \"State.h\"\n+#include \"utils/MathUtils.h\"\n+#include \"StackTrace.h\"\n+#include \"Settings.h\"\n+#include \"Memory.h\"\n+#include \"GCThread.h\"\n+#include \"Sweeper.h\"\n+#include <memory.h>\n+#include <time.h>\n+#include <inttypes.h>\n+#include <sched.h>\n+\n+// Allow read and write\n+#define HEAP_MEM_PROT (PROT_READ | PROT_WRITE)\n+// Map private anonymous memory, and prevent from reserving swap\n+#define HEAP_MEM_FLAGS (MAP_NORESERVE | MAP_PRIVATE | MAP_ANONYMOUS)\n+// Map anonymous memory (not a file)\n+#define HEAP_MEM_FD -1\n+#define HEAP_MEM_FD_OFFSET 0\n+\n+void Heap_exitWithOutOfMemory() {\n+    printf(\"Out of heap space\\n\");\n+    StackTrace_PrintStackTrace();\n+    exit(1);\n+}\n+\n+bool Heap_isGrowingPossible(Heap *heap, uint32_t incrementInBlocks) {\n+    return heap->blockCount + incrementInBlocks <= heap->maxBlockCount;\n+}\n+\n+size_t Heap_getMemoryLimit() {\n+    size_t memorySize = getMemorySize();\n+    if ((uint64_t)memorySize > MAX_HEAP_SIZE) {\n+        return (size_t)MAX_HEAP_SIZE;\n+    } else {\n+        return memorySize;\n+    }\n+}\n+\n+/**\n+ * Maps `MAX_SIZE` of memory and returns the first address aligned on\n+ * `alignement` mask\n+ */\n+word_t *Heap_mapAndAlign(size_t memoryLimit, size_t alignmentSize) {\n+    assert(alignmentSize % WORD_SIZE == 0);\n+    word_t *heapStart = mmap(NULL, memoryLimit, HEAP_MEM_PROT, HEAP_MEM_FLAGS,\n+                             HEAP_MEM_FD, HEAP_MEM_FD_OFFSET);\n+\n+    size_t alignmentMask = ~(alignmentSize - 1);\n+    // Heap start not aligned on\n+    if (((word_t)heapStart & alignmentMask) != (word_t)heapStart) {\n+        word_t *previousBlock = (word_t *)((word_t)heapStart & alignmentMask);\n+        heapStart = previousBlock + alignmentSize / WORD_SIZE;\n+    }\n+    return heapStart;\n+}\n+\n+/**\n+ * Allocates the heap struct and initializes it\n+ */\n+void Heap_Init(Heap *heap, size_t minHeapSize, size_t maxHeapSize) {\n+    size_t memoryLimit = Heap_getMemoryLimit();\n+\n+    if (maxHeapSize < MIN_HEAP_SIZE) {\n+        fprintf(stderr,\n+                \"SCALANATIVE_MAX_HEAP_SIZE too small to initialize heap.\\n\");\n+        fprintf(stderr, \"Minimum required: %zum \\n\",\n+                MIN_HEAP_SIZE / 1024 / 1024);\n+        fflush(stderr);\n+        exit(1);\n+    }\n+\n+    if (minHeapSize > memoryLimit) {\n+        fprintf(stderr, \"SCALANATIVE_MIN_HEAP_SIZE is too large.\\n\");\n+        fprintf(stderr, \"Maximum possible: %zug \\n\",\n+                memoryLimit / 1024 / 1024 / 1024);\n+        fflush(stderr);\n+        exit(1);\n+    }\n+\n+    if (maxHeapSize < minHeapSize) {\n+        fprintf(stderr, \"SCALANATIVE_MAX_HEAP_SIZE should be at least \"\n+                        \"SCALANATIVE_MIN_HEAP_SIZE\\n\");\n+        fflush(stderr);\n+        exit(1);\n+    }\n+\n+    if (minHeapSize < MIN_HEAP_SIZE) {\n+        minHeapSize = MIN_HEAP_SIZE;\n+    }\n+\n+    if (maxHeapSize == UNLIMITED_HEAP_SIZE) {\n+        maxHeapSize = memoryLimit;\n+    }\n+\n+    uint32_t maxNumberOfBlocks = maxHeapSize / SPACE_USED_PER_BLOCK;\n+    uint32_t initialBlockCount = minHeapSize / SPACE_USED_PER_BLOCK;\n+    heap->maxHeapSize = maxHeapSize;\n+    heap->blockCount = initialBlockCount;\n+    heap->maxBlockCount = maxNumberOfBlocks;\n+\n+    // reserve space for block headers\n+    size_t blockMetaSpaceSize = maxNumberOfBlocks * sizeof(BlockMeta);\n+    word_t *blockMetaStart = Heap_mapAndAlign(blockMetaSpaceSize, WORD_SIZE);\n+    heap->blockMetaStart = blockMetaStart;\n+    heap->blockMetaEnd =\n+        blockMetaStart + initialBlockCount * sizeof(BlockMeta) / WORD_SIZE;\n+\n+    // reserve space for line headers\n+    size_t lineMetaSpaceSize =\n+        (size_t)maxNumberOfBlocks * LINE_COUNT * LINE_METADATA_SIZE;\n+    word_t *lineMetaStart = Heap_mapAndAlign(lineMetaSpaceSize, WORD_SIZE);\n+    heap->lineMetaStart = lineMetaStart;\n+    assert(LINE_COUNT * LINE_SIZE == BLOCK_TOTAL_SIZE);\n+    assert(LINE_COUNT * LINE_METADATA_SIZE % WORD_SIZE == 0);\n+    heap->lineMetaEnd = lineMetaStart + initialBlockCount * LINE_COUNT *\n+                                            LINE_METADATA_SIZE / WORD_SIZE;\n+\n+    word_t *heapStart = Heap_mapAndAlign(maxHeapSize, BLOCK_TOTAL_SIZE);\n+\n+    BlockAllocator_Init(&blockAllocator, blockMetaStart, initialBlockCount);\n+    GreyList_Init(&heap->mark.empty);\n+    GreyList_Init(&heap->mark.full);\n+    uint32_t greyPacketCount = (uint32_t)(maxHeapSize * GREY_PACKET_RATIO / GREY_PACKET_SIZE);\n+    heap->mark.total = greyPacketCount;\n+    word_t* greyPacketsStart = Heap_mapAndAlign(greyPacketCount * sizeof(GreyPacket), WORD_SIZE);\n+    heap->greyPacketsStart = greyPacketsStart;\n+    GreyList_PushAll(&heap->mark.empty, greyPacketsStart, (GreyPacket *) greyPacketsStart, greyPacketCount);\n+\n+    // reserve space for bytemap\n+    Bytemap *bytemap = (Bytemap *)Heap_mapAndAlign(\n+        maxHeapSize / ALLOCATION_ALIGNMENT + sizeof(Bytemap),\n+        ALLOCATION_ALIGNMENT);\n+    heap->bytemap = bytemap;\n+\n+    // Init heap for small objects\n+    heap->heapSize = minHeapSize;\n+    heap->heapStart = heapStart;\n+    heap->heapEnd = heapStart + minHeapSize / WORD_SIZE;\n+    heap->sweep.cursor = initialBlockCount;\n+    heap->lazySweep.cursorDone = initialBlockCount;\n+    heap->sweep.limit = initialBlockCount;\n+    heap->sweep.coalesceDone = initialBlockCount;\n+    heap->sweep.postSweepDone = true;\n+    Bytemap_Init(bytemap, heapStart, maxHeapSize);\n+    Allocator_Init(&allocator, &blockAllocator, bytemap, blockMetaStart,\n+                   heapStart);\n+\n+    LargeAllocator_Init(&largeAllocator, &blockAllocator, bytemap,\n+                        blockMetaStart, heapStart);\n+\n+    // Init all GCThreads\n+    sem_init(&heap->gcThreads.start, 0, 0);\n+    sem_init(&heap->gcThreads.start0, 0, 0);\n+\n+    // Init stats if enabled.\n+    // This must done before initializing other threads.\n+#ifdef ENABLE_GC_STATS\n+    char *statsFile = Settings_StatsFileName();\n+    if (statsFile != NULL) {\n+        heap->stats = malloc(sizeof(Stats));\n+        Stats_Init(heap->stats, statsFile, MUTATOR_THREAD_ID);\n+    } else {\n+#endif\n+        heap->stats = NULL;\n+#ifdef ENABLE_GC_STATS\n+    }\n+#endif\n+\n+\n+    int gcThreadCount = Settings_GCThreadCount();\n+    heap->gcThreads.count = gcThreadCount;\n+    heap->gcThreads.phase = gc_idle;\n+    GCThread *gcThreads = (GCThread *)malloc(sizeof(GCThread) * gcThreadCount);\n+    heap->gcThreads.all = (void *)gcThreads;\n+    for (int i = 0; i < gcThreadCount; i++) {\n+        Stats *stats;\n+#ifdef ENABLE_GC_STATS\n+        if (statsFile != NULL) {\n+            int len = strlen(statsFile) + 5;\n+            char *threadSpecificFile = (char *) malloc(len);\n+            snprintf(threadSpecificFile, len, \"%s.t%d\", statsFile, i);\n+            stats = malloc(sizeof(Stats));\n+            Stats_Init(stats, threadSpecificFile, (uint8_t)i);\n+        } else {\n+#endif\n+            stats = NULL;\n+#ifdef ENABLE_GC_STATS\n+        }\n+#endif\n+        GCThread_Init(&gcThreads[i], i, heap, stats);\n+    }\n+\n+    heap->mark.lastEnd_ns = scalanative_nano_time();\n+\n+    pthread_mutex_init(&heap->sweep.growMutex, NULL);\n+}\n+\n+word_t *Heap_AllocLarge(Heap *heap, uint32_t size) {\n+\n+    assert(size % ALLOCATION_ALIGNMENT == 0);\n+    assert(size >= MIN_BLOCK_SIZE);\n+\n+    Object *object = LargeAllocator_GetBlock(&largeAllocator, size);\n+    if (object != NULL) {\n+done:\n+        assert(object != NULL);\n+        assert(Heap_IsWordInHeap(heap, (word_t *)object));\n+        return (word_t *)object;\n+}\n+\n+    if (!Sweeper_IsSweepDone(heap)) {\n+        object = Sweeper_LazySweepLarge(heap, size);\n+        if (object != NULL)\n+            goto done;\n+    }\n+\n+    Heap_Collect(heap);\n+\n+    object = LargeAllocator_GetBlock(&largeAllocator, size);\n+    if (object != NULL)\n+        goto done;\n+\n+    if (!Sweeper_IsSweepDone(heap)) {\n+        object = Sweeper_LazySweepLarge(heap, size);\n+        if (object != NULL)\n+            goto done;\n+    }\n+\n+    size_t increment = MathUtils_DivAndRoundUp(size, BLOCK_TOTAL_SIZE);\n+    uint32_t pow2increment = 1U << MathUtils_Log2Ceil(increment);\n+    Heap_Grow(heap, pow2increment);\n+\n+    object = LargeAllocator_GetBlock(&largeAllocator, size);\n+\n+    goto done;\n+}\n+\n+NOINLINE word_t *Heap_allocSmallSlow(Heap *heap, uint32_t size) {\n+    Object *object = (Object *) Allocator_Alloc(&allocator, size);\n+\n+    if (object != NULL) {\n+done:\n+        assert(Heap_IsWordInHeap(heap, (word_t *)object));\n+        assert(object != NULL);\n+        ObjectMeta *objectMeta = Bytemap_Get(allocator.bytemap, (word_t *)object);\n+#ifdef DEBUG_ASSERT\n+        ObjectMeta_AssertIsValidAllocation(objectMeta, size);\n+#endif\n+        ObjectMeta_SetAllocated(objectMeta);\n+        return (word_t *)object;\n+    }\n+\n+    if (!Sweeper_IsSweepDone(heap)) {\n+        object = Sweeper_LazySweep(heap, size);\n+\n+        if (object != NULL)\n+            goto done;\n+    }\n+\n+    Heap_Collect(heap);\n+    object = (Object *) Allocator_Alloc(&allocator, size);\n+\n+    if (object != NULL)\n+        goto done;\n+\n+    if (!Sweeper_IsSweepDone(heap)) {\n+        object = Sweeper_LazySweep(heap, size);\n+\n+        if (object != NULL)\n+            goto done;\n+    }\n+\n+    // A small object can always fit in a single free block\n+    // because it is no larger than 8K while the block is 32K.\n+    Heap_Grow(heap, 1);\n+    object = (Object *) Allocator_Alloc(&allocator, size);\n+\n+    goto done;\n+}\n+\n+INLINE word_t *Heap_AllocSmall(Heap *heap, uint32_t size) {\n+    assert(size % ALLOCATION_ALIGNMENT == 0);\n+    assert(size < MIN_BLOCK_SIZE);\n+\n+    word_t *start = allocator.cursor;\n+    word_t *end = (word_t *)((uint8_t *)start + size);\n+\n+    // Checks if the end of the block overlaps with the limit\n+    if (end >= allocator.limit) {\n+        return Heap_allocSmallSlow(heap, size);\n+    }\n+\n+    allocator.cursor = end;\n+\n+    memset(start, 0, size);\n+\n+    Object *object = (Object *)start;\n+    ObjectMeta *objectMeta = Bytemap_Get(allocator.bytemap, (word_t *)object);\n+#ifdef DEBUG_ASSERT\n+    ObjectMeta_AssertIsValidAllocation(objectMeta, size);\n+#endif\n+    ObjectMeta_SetAllocated(objectMeta);\n+\n+    __builtin_prefetch(object + 36, 0, 3);\n+\n+    assert(Heap_IsWordInHeap(heap, (word_t *)object));\n+    return (word_t *)object;\n+}\n+\n+word_t *Heap_Alloc(Heap *heap, uint32_t objectSize) {\n+    assert(objectSize % ALLOCATION_ALIGNMENT == 0);\n+\n+    if (objectSize >= LARGE_BLOCK_SIZE) {\n+        return Heap_AllocLarge(heap, objectSize);\n+    } else {\n+        return Heap_AllocSmall(heap, objectSize);\n+    }\n+}\n+\n+#ifdef DEBUG_ASSERT\n+void Heap_clearIsSwept(Heap *heap) {\n+    BlockMeta *current = (BlockMeta *)heap->blockMetaStart;\n+    BlockMeta *limit = (BlockMeta *)heap->blockMetaEnd;\n+    while (current < limit) {\n+        BlockMeta *reserveFirst = (BlockMeta *) blockAllocator.reservedSuperblock;\n+        BlockMeta *reserveLimit = reserveFirst + SWEEP_RESERVE_BLOCKS;\n+        if (current < reserveFirst || current >= reserveLimit) {\n+            assert(reserveFirst !=  NULL);\n+            current->debugFlag = dbg_must_sweep;\n+        }\n+        current++;\n+    }\n+}\n+\n+void Heap_assertIsConsistent(Heap *heap) {\n+    BlockMeta *current = (BlockMeta *)heap->blockMetaStart;\n+    LineMeta *lineMetas = (LineMeta *)heap->lineMetaStart;\n+    BlockMeta *limit = (BlockMeta *)heap->blockMetaEnd;\n+    ObjectMeta *currentBlockStart = Bytemap_Get(heap->bytemap, heap->heapStart);\n+    while (current < limit) {\n+        assert(!BlockMeta_IsCoalesceMe(current));\n+        assert(!BlockMeta_IsSuperblockStartMe(current));\n+        assert(!BlockMeta_IsSuperblockTail(current));\n+        assert(!BlockMeta_IsMarked(current));\n+\n+        int size = 1;\n+        if (BlockMeta_IsSuperblockStart(current)) {\n+            size = BlockMeta_SuperblockSize(current);\n+        }\n+        BlockMeta *next = current + size;\n+        LineMeta *nextLineMetas = lineMetas + LINE_COUNT * size;\n+        ObjectMeta *nextBlockStart =\n+            currentBlockStart +\n+            (WORDS_IN_BLOCK / ALLOCATION_ALIGNMENT_WORDS) * size;\n+\n+        for (LineMeta *line = lineMetas; line < nextLineMetas; line++) {\n+            assert(!Line_IsMarked(line));\n+        }\n+        for (ObjectMeta *object = currentBlockStart; object < nextBlockStart;\n+             object++) {\n+            assert(!ObjectMeta_IsMarked(object));\n+        }\n+\n+        current = next;\n+        lineMetas = nextLineMetas;\n+        currentBlockStart = nextBlockStart;\n+    }\n+    assert(current == limit);\n+}\n+#endif\n+\n+void Heap_Collect(Heap *heap) {\n+#ifdef ENABLE_GC_STATS\n+    Stats *stats = heap->stats;\n+    if (stats != NULL) {\n+        stats->collection_start_ns = scalanative_nano_time();\n+    }\n+#else\n+    Stats *stats = NULL;\n+#endif\n+    assert(Sweeper_IsSweepDone(heap));\n+#ifdef DEBUG_ASSERT\n+    Heap_clearIsSwept(heap);\n+    Heap_assertIsConsistent(heap);\n+#endif\n+    heap->mark.lastEnd_ns = heap->mark.currentEnd_ns;\n+    heap->mark.currentStart_ns = scalanative_nano_time();\n+    Marker_MarkRoots(heap, stats);\n+    heap->gcThreads.phase = gc_mark;\n+    // make sure the gc phase is propagated\n+    atomic_thread_fence(memory_order_release);\n+    // start main gc thread (0)\n+    sem_post(&heap->gcThreads.start0);\n+    while (!Marker_IsMarkDone(heap)) {\n+        Marker_Mark(heap, stats);\n+        if (!Marker_IsMarkDone(heap)) {\n+            sched_yield();\n+        }\n+    }\n+    heap->gcThreads.phase = gc_idle;\n+    heap->mark.currentEnd_ns = scalanative_nano_time();\n+#ifdef ENABLE_GC_STATS\n+    if (stats != NULL) {\n+        Stats_RecordEvent(stats, event_mark, heap->mark.currentStart_ns,\n+                          heap->mark.currentEnd_ns);\n+    }\n+#endif\n+    Heap_Recycle(heap);\n+}"
  },
  {
    "id" : "689bbf7b-4abb-455d-9f02-78062096e2b7",
    "prId" : 1423,
    "comments" : [
      {
        "id" : "d2e46a2e-c32e-4a6e-b15f-e80165e8afad",
        "parentId" : null,
        "author" : {
          "login" : "densh",
          "name" : null,
          "avatarUrl" : "https://avatars1.githubusercontent.com/u/320966?u=784f6f761f35b8b7f3f787172b468334d6524524&v=4"
        },
        "body" : "Similarly to `Heap_Collect` most of the code either belongs in sweeper or phase management modules.",
        "createdAt" : "2019-02-01T10:16:31Z",
        "updatedAt" : "2019-07-17T09:53:34Z",
        "lastEditedBy" : {
          "login" : "densh",
          "name" : null,
          "avatarUrl" : "https://avatars1.githubusercontent.com/u/320966?u=784f6f761f35b8b7f3f787172b468334d6524524&v=4"
        },
        "tags" : [
        ]
      }
    ],
    "commit" : "9349177c2456e08c91598c5a158c63a30876a4e0",
    "line" : 251,
    "diffHunk" : "@@ -0,0 +1,542 @@\n+#include <stdlib.h>\n+#include <sys/mman.h>\n+#include <stdio.h>\n+#include \"Heap.h\"\n+#include \"Log.h\"\n+#include \"Allocator.h\"\n+#include \"Marker.h\"\n+#include \"State.h\"\n+#include \"utils/MathUtils.h\"\n+#include \"StackTrace.h\"\n+#include \"Settings.h\"\n+#include \"Memory.h\"\n+#include \"GCThread.h\"\n+#include \"Sweeper.h\"\n+#include <memory.h>\n+#include <time.h>\n+#include <inttypes.h>\n+#include <sched.h>\n+\n+// Allow read and write\n+#define HEAP_MEM_PROT (PROT_READ | PROT_WRITE)\n+// Map private anonymous memory, and prevent from reserving swap\n+#define HEAP_MEM_FLAGS (MAP_NORESERVE | MAP_PRIVATE | MAP_ANONYMOUS)\n+// Map anonymous memory (not a file)\n+#define HEAP_MEM_FD -1\n+#define HEAP_MEM_FD_OFFSET 0\n+\n+void Heap_exitWithOutOfMemory() {\n+    printf(\"Out of heap space\\n\");\n+    StackTrace_PrintStackTrace();\n+    exit(1);\n+}\n+\n+bool Heap_isGrowingPossible(Heap *heap, uint32_t incrementInBlocks) {\n+    return heap->blockCount + incrementInBlocks <= heap->maxBlockCount;\n+}\n+\n+size_t Heap_getMemoryLimit() {\n+    size_t memorySize = getMemorySize();\n+    if ((uint64_t)memorySize > MAX_HEAP_SIZE) {\n+        return (size_t)MAX_HEAP_SIZE;\n+    } else {\n+        return memorySize;\n+    }\n+}\n+\n+/**\n+ * Maps `MAX_SIZE` of memory and returns the first address aligned on\n+ * `alignement` mask\n+ */\n+word_t *Heap_mapAndAlign(size_t memoryLimit, size_t alignmentSize) {\n+    assert(alignmentSize % WORD_SIZE == 0);\n+    word_t *heapStart = mmap(NULL, memoryLimit, HEAP_MEM_PROT, HEAP_MEM_FLAGS,\n+                             HEAP_MEM_FD, HEAP_MEM_FD_OFFSET);\n+\n+    size_t alignmentMask = ~(alignmentSize - 1);\n+    // Heap start not aligned on\n+    if (((word_t)heapStart & alignmentMask) != (word_t)heapStart) {\n+        word_t *previousBlock = (word_t *)((word_t)heapStart & alignmentMask);\n+        heapStart = previousBlock + alignmentSize / WORD_SIZE;\n+    }\n+    return heapStart;\n+}\n+\n+/**\n+ * Allocates the heap struct and initializes it\n+ */\n+void Heap_Init(Heap *heap, size_t minHeapSize, size_t maxHeapSize) {\n+    size_t memoryLimit = Heap_getMemoryLimit();\n+\n+    if (maxHeapSize < MIN_HEAP_SIZE) {\n+        fprintf(stderr,\n+                \"SCALANATIVE_MAX_HEAP_SIZE too small to initialize heap.\\n\");\n+        fprintf(stderr, \"Minimum required: %zum \\n\",\n+                MIN_HEAP_SIZE / 1024 / 1024);\n+        fflush(stderr);\n+        exit(1);\n+    }\n+\n+    if (minHeapSize > memoryLimit) {\n+        fprintf(stderr, \"SCALANATIVE_MIN_HEAP_SIZE is too large.\\n\");\n+        fprintf(stderr, \"Maximum possible: %zug \\n\",\n+                memoryLimit / 1024 / 1024 / 1024);\n+        fflush(stderr);\n+        exit(1);\n+    }\n+\n+    if (maxHeapSize < minHeapSize) {\n+        fprintf(stderr, \"SCALANATIVE_MAX_HEAP_SIZE should be at least \"\n+                        \"SCALANATIVE_MIN_HEAP_SIZE\\n\");\n+        fflush(stderr);\n+        exit(1);\n+    }\n+\n+    if (minHeapSize < MIN_HEAP_SIZE) {\n+        minHeapSize = MIN_HEAP_SIZE;\n+    }\n+\n+    if (maxHeapSize == UNLIMITED_HEAP_SIZE) {\n+        maxHeapSize = memoryLimit;\n+    }\n+\n+    uint32_t maxNumberOfBlocks = maxHeapSize / SPACE_USED_PER_BLOCK;\n+    uint32_t initialBlockCount = minHeapSize / SPACE_USED_PER_BLOCK;\n+    heap->maxHeapSize = maxHeapSize;\n+    heap->blockCount = initialBlockCount;\n+    heap->maxBlockCount = maxNumberOfBlocks;\n+\n+    // reserve space for block headers\n+    size_t blockMetaSpaceSize = maxNumberOfBlocks * sizeof(BlockMeta);\n+    word_t *blockMetaStart = Heap_mapAndAlign(blockMetaSpaceSize, WORD_SIZE);\n+    heap->blockMetaStart = blockMetaStart;\n+    heap->blockMetaEnd =\n+        blockMetaStart + initialBlockCount * sizeof(BlockMeta) / WORD_SIZE;\n+\n+    // reserve space for line headers\n+    size_t lineMetaSpaceSize =\n+        (size_t)maxNumberOfBlocks * LINE_COUNT * LINE_METADATA_SIZE;\n+    word_t *lineMetaStart = Heap_mapAndAlign(lineMetaSpaceSize, WORD_SIZE);\n+    heap->lineMetaStart = lineMetaStart;\n+    assert(LINE_COUNT * LINE_SIZE == BLOCK_TOTAL_SIZE);\n+    assert(LINE_COUNT * LINE_METADATA_SIZE % WORD_SIZE == 0);\n+    heap->lineMetaEnd = lineMetaStart + initialBlockCount * LINE_COUNT *\n+                                            LINE_METADATA_SIZE / WORD_SIZE;\n+\n+    word_t *heapStart = Heap_mapAndAlign(maxHeapSize, BLOCK_TOTAL_SIZE);\n+\n+    BlockAllocator_Init(&blockAllocator, blockMetaStart, initialBlockCount);\n+    GreyList_Init(&heap->mark.empty);\n+    GreyList_Init(&heap->mark.full);\n+    uint32_t greyPacketCount = (uint32_t)(maxHeapSize * GREY_PACKET_RATIO / GREY_PACKET_SIZE);\n+    heap->mark.total = greyPacketCount;\n+    word_t* greyPacketsStart = Heap_mapAndAlign(greyPacketCount * sizeof(GreyPacket), WORD_SIZE);\n+    heap->greyPacketsStart = greyPacketsStart;\n+    GreyList_PushAll(&heap->mark.empty, greyPacketsStart, (GreyPacket *) greyPacketsStart, greyPacketCount);\n+\n+    // reserve space for bytemap\n+    Bytemap *bytemap = (Bytemap *)Heap_mapAndAlign(\n+        maxHeapSize / ALLOCATION_ALIGNMENT + sizeof(Bytemap),\n+        ALLOCATION_ALIGNMENT);\n+    heap->bytemap = bytemap;\n+\n+    // Init heap for small objects\n+    heap->heapSize = minHeapSize;\n+    heap->heapStart = heapStart;\n+    heap->heapEnd = heapStart + minHeapSize / WORD_SIZE;\n+    heap->sweep.cursor = initialBlockCount;\n+    heap->lazySweep.cursorDone = initialBlockCount;\n+    heap->sweep.limit = initialBlockCount;\n+    heap->sweep.coalesceDone = initialBlockCount;\n+    heap->sweep.postSweepDone = true;\n+    Bytemap_Init(bytemap, heapStart, maxHeapSize);\n+    Allocator_Init(&allocator, &blockAllocator, bytemap, blockMetaStart,\n+                   heapStart);\n+\n+    LargeAllocator_Init(&largeAllocator, &blockAllocator, bytemap,\n+                        blockMetaStart, heapStart);\n+\n+    // Init all GCThreads\n+    sem_init(&heap->gcThreads.start, 0, 0);\n+    sem_init(&heap->gcThreads.start0, 0, 0);\n+\n+    // Init stats if enabled.\n+    // This must done before initializing other threads.\n+#ifdef ENABLE_GC_STATS\n+    char *statsFile = Settings_StatsFileName();\n+    if (statsFile != NULL) {\n+        heap->stats = malloc(sizeof(Stats));\n+        Stats_Init(heap->stats, statsFile, MUTATOR_THREAD_ID);\n+    } else {\n+#endif\n+        heap->stats = NULL;\n+#ifdef ENABLE_GC_STATS\n+    }\n+#endif\n+\n+\n+    int gcThreadCount = Settings_GCThreadCount();\n+    heap->gcThreads.count = gcThreadCount;\n+    heap->gcThreads.phase = gc_idle;\n+    GCThread *gcThreads = (GCThread *)malloc(sizeof(GCThread) * gcThreadCount);\n+    heap->gcThreads.all = (void *)gcThreads;\n+    for (int i = 0; i < gcThreadCount; i++) {\n+        Stats *stats;\n+#ifdef ENABLE_GC_STATS\n+        if (statsFile != NULL) {\n+            int len = strlen(statsFile) + 5;\n+            char *threadSpecificFile = (char *) malloc(len);\n+            snprintf(threadSpecificFile, len, \"%s.t%d\", statsFile, i);\n+            stats = malloc(sizeof(Stats));\n+            Stats_Init(stats, threadSpecificFile, (uint8_t)i);\n+        } else {\n+#endif\n+            stats = NULL;\n+#ifdef ENABLE_GC_STATS\n+        }\n+#endif\n+        GCThread_Init(&gcThreads[i], i, heap, stats);\n+    }\n+\n+    heap->mark.lastEnd_ns = scalanative_nano_time();\n+\n+    pthread_mutex_init(&heap->sweep.growMutex, NULL);\n+}\n+\n+word_t *Heap_AllocLarge(Heap *heap, uint32_t size) {\n+\n+    assert(size % ALLOCATION_ALIGNMENT == 0);\n+    assert(size >= MIN_BLOCK_SIZE);\n+\n+    Object *object = LargeAllocator_GetBlock(&largeAllocator, size);\n+    if (object != NULL) {\n+done:\n+        assert(object != NULL);\n+        assert(Heap_IsWordInHeap(heap, (word_t *)object));\n+        return (word_t *)object;\n+}\n+\n+    if (!Sweeper_IsSweepDone(heap)) {\n+        object = Sweeper_LazySweepLarge(heap, size);\n+        if (object != NULL)\n+            goto done;\n+    }\n+\n+    Heap_Collect(heap);\n+\n+    object = LargeAllocator_GetBlock(&largeAllocator, size);\n+    if (object != NULL)\n+        goto done;\n+\n+    if (!Sweeper_IsSweepDone(heap)) {\n+        object = Sweeper_LazySweepLarge(heap, size);\n+        if (object != NULL)\n+            goto done;\n+    }\n+\n+    size_t increment = MathUtils_DivAndRoundUp(size, BLOCK_TOTAL_SIZE);\n+    uint32_t pow2increment = 1U << MathUtils_Log2Ceil(increment);\n+    Heap_Grow(heap, pow2increment);\n+\n+    object = LargeAllocator_GetBlock(&largeAllocator, size);\n+\n+    goto done;\n+}\n+\n+NOINLINE word_t *Heap_allocSmallSlow(Heap *heap, uint32_t size) {\n+    Object *object = (Object *) Allocator_Alloc(&allocator, size);\n+\n+    if (object != NULL) {\n+done:\n+        assert(Heap_IsWordInHeap(heap, (word_t *)object));\n+        assert(object != NULL);\n+        ObjectMeta *objectMeta = Bytemap_Get(allocator.bytemap, (word_t *)object);\n+#ifdef DEBUG_ASSERT\n+        ObjectMeta_AssertIsValidAllocation(objectMeta, size);\n+#endif\n+        ObjectMeta_SetAllocated(objectMeta);\n+        return (word_t *)object;\n+    }\n+\n+    if (!Sweeper_IsSweepDone(heap)) {\n+        object = Sweeper_LazySweep(heap, size);\n+\n+        if (object != NULL)\n+            goto done;\n+    }\n+\n+    Heap_Collect(heap);\n+    object = (Object *) Allocator_Alloc(&allocator, size);\n+\n+    if (object != NULL)\n+        goto done;\n+\n+    if (!Sweeper_IsSweepDone(heap)) {\n+        object = Sweeper_LazySweep(heap, size);\n+\n+        if (object != NULL)\n+            goto done;\n+    }\n+\n+    // A small object can always fit in a single free block\n+    // because it is no larger than 8K while the block is 32K.\n+    Heap_Grow(heap, 1);\n+    object = (Object *) Allocator_Alloc(&allocator, size);\n+\n+    goto done;\n+}\n+\n+INLINE word_t *Heap_AllocSmall(Heap *heap, uint32_t size) {\n+    assert(size % ALLOCATION_ALIGNMENT == 0);\n+    assert(size < MIN_BLOCK_SIZE);\n+\n+    word_t *start = allocator.cursor;\n+    word_t *end = (word_t *)((uint8_t *)start + size);\n+\n+    // Checks if the end of the block overlaps with the limit\n+    if (end >= allocator.limit) {\n+        return Heap_allocSmallSlow(heap, size);\n+    }\n+\n+    allocator.cursor = end;\n+\n+    memset(start, 0, size);\n+\n+    Object *object = (Object *)start;\n+    ObjectMeta *objectMeta = Bytemap_Get(allocator.bytemap, (word_t *)object);\n+#ifdef DEBUG_ASSERT\n+    ObjectMeta_AssertIsValidAllocation(objectMeta, size);\n+#endif\n+    ObjectMeta_SetAllocated(objectMeta);\n+\n+    __builtin_prefetch(object + 36, 0, 3);\n+\n+    assert(Heap_IsWordInHeap(heap, (word_t *)object));\n+    return (word_t *)object;\n+}\n+\n+word_t *Heap_Alloc(Heap *heap, uint32_t objectSize) {\n+    assert(objectSize % ALLOCATION_ALIGNMENT == 0);\n+\n+    if (objectSize >= LARGE_BLOCK_SIZE) {\n+        return Heap_AllocLarge(heap, objectSize);\n+    } else {\n+        return Heap_AllocSmall(heap, objectSize);\n+    }\n+}\n+\n+#ifdef DEBUG_ASSERT\n+void Heap_clearIsSwept(Heap *heap) {\n+    BlockMeta *current = (BlockMeta *)heap->blockMetaStart;\n+    BlockMeta *limit = (BlockMeta *)heap->blockMetaEnd;\n+    while (current < limit) {\n+        BlockMeta *reserveFirst = (BlockMeta *) blockAllocator.reservedSuperblock;\n+        BlockMeta *reserveLimit = reserveFirst + SWEEP_RESERVE_BLOCKS;\n+        if (current < reserveFirst || current >= reserveLimit) {\n+            assert(reserveFirst !=  NULL);\n+            current->debugFlag = dbg_must_sweep;\n+        }\n+        current++;\n+    }\n+}\n+\n+void Heap_assertIsConsistent(Heap *heap) {\n+    BlockMeta *current = (BlockMeta *)heap->blockMetaStart;\n+    LineMeta *lineMetas = (LineMeta *)heap->lineMetaStart;\n+    BlockMeta *limit = (BlockMeta *)heap->blockMetaEnd;\n+    ObjectMeta *currentBlockStart = Bytemap_Get(heap->bytemap, heap->heapStart);\n+    while (current < limit) {\n+        assert(!BlockMeta_IsCoalesceMe(current));\n+        assert(!BlockMeta_IsSuperblockStartMe(current));\n+        assert(!BlockMeta_IsSuperblockTail(current));\n+        assert(!BlockMeta_IsMarked(current));\n+\n+        int size = 1;\n+        if (BlockMeta_IsSuperblockStart(current)) {\n+            size = BlockMeta_SuperblockSize(current);\n+        }\n+        BlockMeta *next = current + size;\n+        LineMeta *nextLineMetas = lineMetas + LINE_COUNT * size;\n+        ObjectMeta *nextBlockStart =\n+            currentBlockStart +\n+            (WORDS_IN_BLOCK / ALLOCATION_ALIGNMENT_WORDS) * size;\n+\n+        for (LineMeta *line = lineMetas; line < nextLineMetas; line++) {\n+            assert(!Line_IsMarked(line));\n+        }\n+        for (ObjectMeta *object = currentBlockStart; object < nextBlockStart;\n+             object++) {\n+            assert(!ObjectMeta_IsMarked(object));\n+        }\n+\n+        current = next;\n+        lineMetas = nextLineMetas;\n+        currentBlockStart = nextBlockStart;\n+    }\n+    assert(current == limit);\n+}\n+#endif\n+\n+void Heap_Collect(Heap *heap) {\n+#ifdef ENABLE_GC_STATS\n+    Stats *stats = heap->stats;\n+    if (stats != NULL) {\n+        stats->collection_start_ns = scalanative_nano_time();\n+    }\n+#else\n+    Stats *stats = NULL;\n+#endif\n+    assert(Sweeper_IsSweepDone(heap));\n+#ifdef DEBUG_ASSERT\n+    Heap_clearIsSwept(heap);\n+    Heap_assertIsConsistent(heap);\n+#endif\n+    heap->mark.lastEnd_ns = heap->mark.currentEnd_ns;\n+    heap->mark.currentStart_ns = scalanative_nano_time();\n+    Marker_MarkRoots(heap, stats);\n+    heap->gcThreads.phase = gc_mark;\n+    // make sure the gc phase is propagated\n+    atomic_thread_fence(memory_order_release);\n+    // start main gc thread (0)\n+    sem_post(&heap->gcThreads.start0);\n+    while (!Marker_IsMarkDone(heap)) {\n+        Marker_Mark(heap, stats);\n+        if (!Marker_IsMarkDone(heap)) {\n+            sched_yield();\n+        }\n+    }\n+    heap->gcThreads.phase = gc_idle;\n+    heap->mark.currentEnd_ns = scalanative_nano_time();\n+#ifdef ENABLE_GC_STATS\n+    if (stats != NULL) {\n+        Stats_RecordEvent(stats, event_mark, heap->mark.currentStart_ns,\n+                          heap->mark.currentEnd_ns);\n+    }\n+#endif\n+    Heap_Recycle(heap);\n+}\n+\n+bool Heap_shouldGrow(Heap *heap) {\n+    uint32_t freeBlockCount = (uint32_t)blockAllocator.freeBlockCount;\n+    uint32_t blockCount = heap->blockCount;\n+    uint32_t recycledBlockCount = (uint32_t)allocator.recycledBlockCount;\n+    uint32_t unavailableBlockCount =\n+        blockCount - (freeBlockCount + recycledBlockCount);\n+\n+#ifdef DEBUG_PRINT\n+    printf(\"\\n\\nBlock count: %\" PRIu32 \"\\n\", blockCount);\n+    printf(\"Unavailable: %\" PRIu32 \"\\n\", unavailableBlockCount);\n+    printf(\"Free: %\" PRIu32 \"\\n\", freeBlockCount);\n+    printf(\"Recycled: %\" PRIu32 \"\\n\", recycledBlockCount);\n+    fflush(stdout);\n+#endif\n+\n+    uint64_t timeInMark = heap->mark.currentEnd_ns - heap->mark.currentStart_ns;\n+    uint64_t timeTotal = heap->mark.currentEnd_ns - heap->mark.lastEnd_ns;\n+\n+    return timeInMark >= GROWTH_MARK_FRACTION * timeTotal || freeBlockCount * 2 < blockCount ||\n+           4 * unavailableBlockCount > blockCount;\n+}\n+\n+void Heap_Recycle(Heap *heap) {\n+    Allocator_Clear(&allocator);\n+    LargeAllocator_Clear(&largeAllocator);\n+    BlockAllocator_Clear(&blockAllocator);\n+\n+    // use the reserved block so mutator can does not have to lazy sweep\n+    // but can allocate imminently\n+    BlockAllocator_UseReserve(&blockAllocator);\n+\n+    // all the marking changes should be visible to all threads by now\n+    atomic_thread_fence(memory_order_seq_cst);\n+\n+    // before changing the cursor and limit values, makes sure no gc threads are\n+    // running\n+    GCThread_JoinAll(heap);\n+\n+    heap->sweep.cursor = 0;\n+    uint32_t blockCount = heap->blockCount;\n+    heap->sweep.limit = blockCount;\n+    heap->lazySweep.cursorDone = 0;\n+    // mark as unitialized\n+    heap->lazySweep.lastActivity = BlockRange_Pack(2, 0);\n+    heap->lazySweep.lastActivityObserved = BlockRange_Pack(2, 0);\n+    heap->sweep.coalesceDone = 0;\n+    heap->sweep.postSweepDone = false;\n+\n+    heap->gcThreads.phase = gc_sweep;\n+    // make sure all running parameters are propagated\n+    atomic_thread_fence(memory_order_release);\n+    // determine how many threads need to start\n+    int gcThreadCount = heap->gcThreads.count;\n+    int numberOfBatches = blockCount / SWEEP_BATCH_SIZE;\n+    int threadsToStart = numberOfBatches / MIN_SWEEP_BATCHES_PER_THREAD;\n+    if (threadsToStart <= 0) {\n+        threadsToStart = 1;\n+    }\n+    if (threadsToStart > gcThreadCount){\n+        threadsToStart = gcThreadCount;\n+    }\n+    GCThread_Wake(heap, threadsToStart);\n+}"
  },
  {
    "id" : "eab38e58-d27d-4af3-a345-06bccd771405",
    "prId" : 1423,
    "comments" : [
      {
        "id" : "ed9165ee-f52d-418d-aedb-1fc155cd8e99",
        "parentId" : null,
        "author" : {
          "login" : "densh",
          "name" : null,
          "avatarUrl" : "https://avatars1.githubusercontent.com/u/320966?u=784f6f761f35b8b7f3f787172b468334d6524524&v=4"
        },
        "body" : "This looks like you're going to wait here *on the mutator thread*, is there any way to avoid that? ",
        "createdAt" : "2019-02-01T10:18:06Z",
        "updatedAt" : "2019-07-17T09:53:34Z",
        "lastEditedBy" : {
          "login" : "densh",
          "name" : null,
          "avatarUrl" : "https://avatars1.githubusercontent.com/u/320966?u=784f6f761f35b8b7f3f787172b468334d6524524&v=4"
        },
        "tags" : [
          {
            "value" : "outdated"
          }
        ]
      }
    ],
    "commit" : "9349177c2456e08c91598c5a158c63a30876a4e0",
    "line" : null,
    "diffHunk" : "@@ -0,0 +1,542 @@\n+#include <stdlib.h>\n+#include <sys/mman.h>\n+#include <stdio.h>\n+#include \"Heap.h\"\n+#include \"Log.h\"\n+#include \"Allocator.h\"\n+#include \"Marker.h\"\n+#include \"State.h\"\n+#include \"utils/MathUtils.h\"\n+#include \"StackTrace.h\"\n+#include \"Settings.h\"\n+#include \"Memory.h\"\n+#include \"GCThread.h\"\n+#include \"Sweeper.h\"\n+#include <memory.h>\n+#include <time.h>\n+#include <inttypes.h>\n+#include <sched.h>\n+\n+// Allow read and write\n+#define HEAP_MEM_PROT (PROT_READ | PROT_WRITE)\n+// Map private anonymous memory, and prevent from reserving swap\n+#define HEAP_MEM_FLAGS (MAP_NORESERVE | MAP_PRIVATE | MAP_ANONYMOUS)\n+// Map anonymous memory (not a file)\n+#define HEAP_MEM_FD -1\n+#define HEAP_MEM_FD_OFFSET 0\n+\n+void Heap_exitWithOutOfMemory() {\n+    printf(\"Out of heap space\\n\");\n+    StackTrace_PrintStackTrace();\n+    exit(1);\n+}\n+\n+bool Heap_isGrowingPossible(Heap *heap, uint32_t incrementInBlocks) {\n+    return heap->blockCount + incrementInBlocks <= heap->maxBlockCount;\n+}\n+\n+size_t Heap_getMemoryLimit() {\n+    size_t memorySize = getMemorySize();\n+    if ((uint64_t)memorySize > MAX_HEAP_SIZE) {\n+        return (size_t)MAX_HEAP_SIZE;\n+    } else {\n+        return memorySize;\n+    }\n+}\n+\n+/**\n+ * Maps `MAX_SIZE` of memory and returns the first address aligned on\n+ * `alignement` mask\n+ */\n+word_t *Heap_mapAndAlign(size_t memoryLimit, size_t alignmentSize) {\n+    assert(alignmentSize % WORD_SIZE == 0);\n+    word_t *heapStart = mmap(NULL, memoryLimit, HEAP_MEM_PROT, HEAP_MEM_FLAGS,\n+                             HEAP_MEM_FD, HEAP_MEM_FD_OFFSET);\n+\n+    size_t alignmentMask = ~(alignmentSize - 1);\n+    // Heap start not aligned on\n+    if (((word_t)heapStart & alignmentMask) != (word_t)heapStart) {\n+        word_t *previousBlock = (word_t *)((word_t)heapStart & alignmentMask);\n+        heapStart = previousBlock + alignmentSize / WORD_SIZE;\n+    }\n+    return heapStart;\n+}\n+\n+/**\n+ * Allocates the heap struct and initializes it\n+ */\n+void Heap_Init(Heap *heap, size_t minHeapSize, size_t maxHeapSize) {\n+    size_t memoryLimit = Heap_getMemoryLimit();\n+\n+    if (maxHeapSize < MIN_HEAP_SIZE) {\n+        fprintf(stderr,\n+                \"SCALANATIVE_MAX_HEAP_SIZE too small to initialize heap.\\n\");\n+        fprintf(stderr, \"Minimum required: %zum \\n\",\n+                MIN_HEAP_SIZE / 1024 / 1024);\n+        fflush(stderr);\n+        exit(1);\n+    }\n+\n+    if (minHeapSize > memoryLimit) {\n+        fprintf(stderr, \"SCALANATIVE_MIN_HEAP_SIZE is too large.\\n\");\n+        fprintf(stderr, \"Maximum possible: %zug \\n\",\n+                memoryLimit / 1024 / 1024 / 1024);\n+        fflush(stderr);\n+        exit(1);\n+    }\n+\n+    if (maxHeapSize < minHeapSize) {\n+        fprintf(stderr, \"SCALANATIVE_MAX_HEAP_SIZE should be at least \"\n+                        \"SCALANATIVE_MIN_HEAP_SIZE\\n\");\n+        fflush(stderr);\n+        exit(1);\n+    }\n+\n+    if (minHeapSize < MIN_HEAP_SIZE) {\n+        minHeapSize = MIN_HEAP_SIZE;\n+    }\n+\n+    if (maxHeapSize == UNLIMITED_HEAP_SIZE) {\n+        maxHeapSize = memoryLimit;\n+    }\n+\n+    uint32_t maxNumberOfBlocks = maxHeapSize / SPACE_USED_PER_BLOCK;\n+    uint32_t initialBlockCount = minHeapSize / SPACE_USED_PER_BLOCK;\n+    heap->maxHeapSize = maxHeapSize;\n+    heap->blockCount = initialBlockCount;\n+    heap->maxBlockCount = maxNumberOfBlocks;\n+\n+    // reserve space for block headers\n+    size_t blockMetaSpaceSize = maxNumberOfBlocks * sizeof(BlockMeta);\n+    word_t *blockMetaStart = Heap_mapAndAlign(blockMetaSpaceSize, WORD_SIZE);\n+    heap->blockMetaStart = blockMetaStart;\n+    heap->blockMetaEnd =\n+        blockMetaStart + initialBlockCount * sizeof(BlockMeta) / WORD_SIZE;\n+\n+    // reserve space for line headers\n+    size_t lineMetaSpaceSize =\n+        (size_t)maxNumberOfBlocks * LINE_COUNT * LINE_METADATA_SIZE;\n+    word_t *lineMetaStart = Heap_mapAndAlign(lineMetaSpaceSize, WORD_SIZE);\n+    heap->lineMetaStart = lineMetaStart;\n+    assert(LINE_COUNT * LINE_SIZE == BLOCK_TOTAL_SIZE);\n+    assert(LINE_COUNT * LINE_METADATA_SIZE % WORD_SIZE == 0);\n+    heap->lineMetaEnd = lineMetaStart + initialBlockCount * LINE_COUNT *\n+                                            LINE_METADATA_SIZE / WORD_SIZE;\n+\n+    word_t *heapStart = Heap_mapAndAlign(maxHeapSize, BLOCK_TOTAL_SIZE);\n+\n+    BlockAllocator_Init(&blockAllocator, blockMetaStart, initialBlockCount);\n+    GreyList_Init(&heap->mark.empty);\n+    GreyList_Init(&heap->mark.full);\n+    uint32_t greyPacketCount = (uint32_t)(maxHeapSize * GREY_PACKET_RATIO / GREY_PACKET_SIZE);\n+    heap->mark.total = greyPacketCount;\n+    word_t* greyPacketsStart = Heap_mapAndAlign(greyPacketCount * sizeof(GreyPacket), WORD_SIZE);\n+    heap->greyPacketsStart = greyPacketsStart;\n+    GreyList_PushAll(&heap->mark.empty, greyPacketsStart, (GreyPacket *) greyPacketsStart, greyPacketCount);\n+\n+    // reserve space for bytemap\n+    Bytemap *bytemap = (Bytemap *)Heap_mapAndAlign(\n+        maxHeapSize / ALLOCATION_ALIGNMENT + sizeof(Bytemap),\n+        ALLOCATION_ALIGNMENT);\n+    heap->bytemap = bytemap;\n+\n+    // Init heap for small objects\n+    heap->heapSize = minHeapSize;\n+    heap->heapStart = heapStart;\n+    heap->heapEnd = heapStart + minHeapSize / WORD_SIZE;\n+    heap->sweep.cursor = initialBlockCount;\n+    heap->lazySweep.cursorDone = initialBlockCount;\n+    heap->sweep.limit = initialBlockCount;\n+    heap->sweep.coalesceDone = initialBlockCount;\n+    heap->sweep.postSweepDone = true;\n+    Bytemap_Init(bytemap, heapStart, maxHeapSize);\n+    Allocator_Init(&allocator, &blockAllocator, bytemap, blockMetaStart,\n+                   heapStart);\n+\n+    LargeAllocator_Init(&largeAllocator, &blockAllocator, bytemap,\n+                        blockMetaStart, heapStart);\n+\n+    // Init all GCThreads\n+    sem_init(&heap->gcThreads.start, 0, 0);\n+    sem_init(&heap->gcThreads.start0, 0, 0);\n+\n+    // Init stats if enabled.\n+    // This must done before initializing other threads.\n+#ifdef ENABLE_GC_STATS\n+    char *statsFile = Settings_StatsFileName();\n+    if (statsFile != NULL) {\n+        heap->stats = malloc(sizeof(Stats));\n+        Stats_Init(heap->stats, statsFile, MUTATOR_THREAD_ID);\n+    } else {\n+#endif\n+        heap->stats = NULL;\n+#ifdef ENABLE_GC_STATS\n+    }\n+#endif\n+\n+\n+    int gcThreadCount = Settings_GCThreadCount();\n+    heap->gcThreads.count = gcThreadCount;\n+    heap->gcThreads.phase = gc_idle;\n+    GCThread *gcThreads = (GCThread *)malloc(sizeof(GCThread) * gcThreadCount);\n+    heap->gcThreads.all = (void *)gcThreads;\n+    for (int i = 0; i < gcThreadCount; i++) {\n+        Stats *stats;\n+#ifdef ENABLE_GC_STATS\n+        if (statsFile != NULL) {\n+            int len = strlen(statsFile) + 5;\n+            char *threadSpecificFile = (char *) malloc(len);\n+            snprintf(threadSpecificFile, len, \"%s.t%d\", statsFile, i);\n+            stats = malloc(sizeof(Stats));\n+            Stats_Init(stats, threadSpecificFile, (uint8_t)i);\n+        } else {\n+#endif\n+            stats = NULL;\n+#ifdef ENABLE_GC_STATS\n+        }\n+#endif\n+        GCThread_Init(&gcThreads[i], i, heap, stats);\n+    }\n+\n+    heap->mark.lastEnd_ns = scalanative_nano_time();\n+\n+    pthread_mutex_init(&heap->sweep.growMutex, NULL);\n+}\n+\n+word_t *Heap_AllocLarge(Heap *heap, uint32_t size) {\n+\n+    assert(size % ALLOCATION_ALIGNMENT == 0);\n+    assert(size >= MIN_BLOCK_SIZE);\n+\n+    Object *object = LargeAllocator_GetBlock(&largeAllocator, size);\n+    if (object != NULL) {\n+done:\n+        assert(object != NULL);\n+        assert(Heap_IsWordInHeap(heap, (word_t *)object));\n+        return (word_t *)object;\n+}\n+\n+    if (!Sweeper_IsSweepDone(heap)) {\n+        object = Sweeper_LazySweepLarge(heap, size);\n+        if (object != NULL)\n+            goto done;\n+    }\n+\n+    Heap_Collect(heap);\n+\n+    object = LargeAllocator_GetBlock(&largeAllocator, size);\n+    if (object != NULL)\n+        goto done;\n+\n+    if (!Sweeper_IsSweepDone(heap)) {\n+        object = Sweeper_LazySweepLarge(heap, size);\n+        if (object != NULL)\n+            goto done;\n+    }\n+\n+    size_t increment = MathUtils_DivAndRoundUp(size, BLOCK_TOTAL_SIZE);\n+    uint32_t pow2increment = 1U << MathUtils_Log2Ceil(increment);\n+    Heap_Grow(heap, pow2increment);\n+\n+    object = LargeAllocator_GetBlock(&largeAllocator, size);\n+\n+    goto done;\n+}\n+\n+NOINLINE word_t *Heap_allocSmallSlow(Heap *heap, uint32_t size) {\n+    Object *object = (Object *) Allocator_Alloc(&allocator, size);\n+\n+    if (object != NULL) {\n+done:\n+        assert(Heap_IsWordInHeap(heap, (word_t *)object));\n+        assert(object != NULL);\n+        ObjectMeta *objectMeta = Bytemap_Get(allocator.bytemap, (word_t *)object);\n+#ifdef DEBUG_ASSERT\n+        ObjectMeta_AssertIsValidAllocation(objectMeta, size);\n+#endif\n+        ObjectMeta_SetAllocated(objectMeta);\n+        return (word_t *)object;\n+    }\n+\n+    if (!Sweeper_IsSweepDone(heap)) {\n+        object = Sweeper_LazySweep(heap, size);\n+\n+        if (object != NULL)\n+            goto done;\n+    }\n+\n+    Heap_Collect(heap);\n+    object = (Object *) Allocator_Alloc(&allocator, size);\n+\n+    if (object != NULL)\n+        goto done;\n+\n+    if (!Sweeper_IsSweepDone(heap)) {\n+        object = Sweeper_LazySweep(heap, size);\n+\n+        if (object != NULL)\n+            goto done;\n+    }\n+\n+    // A small object can always fit in a single free block\n+    // because it is no larger than 8K while the block is 32K.\n+    Heap_Grow(heap, 1);\n+    object = (Object *) Allocator_Alloc(&allocator, size);\n+\n+    goto done;\n+}\n+\n+INLINE word_t *Heap_AllocSmall(Heap *heap, uint32_t size) {\n+    assert(size % ALLOCATION_ALIGNMENT == 0);\n+    assert(size < MIN_BLOCK_SIZE);\n+\n+    word_t *start = allocator.cursor;\n+    word_t *end = (word_t *)((uint8_t *)start + size);\n+\n+    // Checks if the end of the block overlaps with the limit\n+    if (end >= allocator.limit) {\n+        return Heap_allocSmallSlow(heap, size);\n+    }\n+\n+    allocator.cursor = end;\n+\n+    memset(start, 0, size);\n+\n+    Object *object = (Object *)start;\n+    ObjectMeta *objectMeta = Bytemap_Get(allocator.bytemap, (word_t *)object);\n+#ifdef DEBUG_ASSERT\n+    ObjectMeta_AssertIsValidAllocation(objectMeta, size);\n+#endif\n+    ObjectMeta_SetAllocated(objectMeta);\n+\n+    __builtin_prefetch(object + 36, 0, 3);\n+\n+    assert(Heap_IsWordInHeap(heap, (word_t *)object));\n+    return (word_t *)object;\n+}\n+\n+word_t *Heap_Alloc(Heap *heap, uint32_t objectSize) {\n+    assert(objectSize % ALLOCATION_ALIGNMENT == 0);\n+\n+    if (objectSize >= LARGE_BLOCK_SIZE) {\n+        return Heap_AllocLarge(heap, objectSize);\n+    } else {\n+        return Heap_AllocSmall(heap, objectSize);\n+    }\n+}\n+\n+#ifdef DEBUG_ASSERT\n+void Heap_clearIsSwept(Heap *heap) {\n+    BlockMeta *current = (BlockMeta *)heap->blockMetaStart;\n+    BlockMeta *limit = (BlockMeta *)heap->blockMetaEnd;\n+    while (current < limit) {\n+        BlockMeta *reserveFirst = (BlockMeta *) blockAllocator.reservedSuperblock;\n+        BlockMeta *reserveLimit = reserveFirst + SWEEP_RESERVE_BLOCKS;\n+        if (current < reserveFirst || current >= reserveLimit) {\n+            assert(reserveFirst !=  NULL);\n+            current->debugFlag = dbg_must_sweep;\n+        }\n+        current++;\n+    }\n+}\n+\n+void Heap_assertIsConsistent(Heap *heap) {\n+    BlockMeta *current = (BlockMeta *)heap->blockMetaStart;\n+    LineMeta *lineMetas = (LineMeta *)heap->lineMetaStart;\n+    BlockMeta *limit = (BlockMeta *)heap->blockMetaEnd;\n+    ObjectMeta *currentBlockStart = Bytemap_Get(heap->bytemap, heap->heapStart);\n+    while (current < limit) {\n+        assert(!BlockMeta_IsCoalesceMe(current));\n+        assert(!BlockMeta_IsSuperblockStartMe(current));\n+        assert(!BlockMeta_IsSuperblockTail(current));\n+        assert(!BlockMeta_IsMarked(current));\n+\n+        int size = 1;\n+        if (BlockMeta_IsSuperblockStart(current)) {\n+            size = BlockMeta_SuperblockSize(current);\n+        }\n+        BlockMeta *next = current + size;\n+        LineMeta *nextLineMetas = lineMetas + LINE_COUNT * size;\n+        ObjectMeta *nextBlockStart =\n+            currentBlockStart +\n+            (WORDS_IN_BLOCK / ALLOCATION_ALIGNMENT_WORDS) * size;\n+\n+        for (LineMeta *line = lineMetas; line < nextLineMetas; line++) {\n+            assert(!Line_IsMarked(line));\n+        }\n+        for (ObjectMeta *object = currentBlockStart; object < nextBlockStart;\n+             object++) {\n+            assert(!ObjectMeta_IsMarked(object));\n+        }\n+\n+        current = next;\n+        lineMetas = nextLineMetas;\n+        currentBlockStart = nextBlockStart;\n+    }\n+    assert(current == limit);\n+}\n+#endif\n+\n+void Heap_Collect(Heap *heap) {\n+#ifdef ENABLE_GC_STATS\n+    Stats *stats = heap->stats;\n+    if (stats != NULL) {\n+        stats->collection_start_ns = scalanative_nano_time();\n+    }\n+#else\n+    Stats *stats = NULL;\n+#endif\n+    assert(Sweeper_IsSweepDone(heap));\n+#ifdef DEBUG_ASSERT\n+    Heap_clearIsSwept(heap);\n+    Heap_assertIsConsistent(heap);\n+#endif\n+    heap->mark.lastEnd_ns = heap->mark.currentEnd_ns;\n+    heap->mark.currentStart_ns = scalanative_nano_time();\n+    Marker_MarkRoots(heap, stats);\n+    heap->gcThreads.phase = gc_mark;\n+    // make sure the gc phase is propagated\n+    atomic_thread_fence(memory_order_release);\n+    // start main gc thread (0)\n+    sem_post(&heap->gcThreads.start0);\n+    while (!Marker_IsMarkDone(heap)) {\n+        Marker_Mark(heap, stats);\n+        if (!Marker_IsMarkDone(heap)) {\n+            sched_yield();\n+        }\n+    }\n+    heap->gcThreads.phase = gc_idle;\n+    heap->mark.currentEnd_ns = scalanative_nano_time();\n+#ifdef ENABLE_GC_STATS\n+    if (stats != NULL) {\n+        Stats_RecordEvent(stats, event_mark, heap->mark.currentStart_ns,\n+                          heap->mark.currentEnd_ns);\n+    }\n+#endif\n+    Heap_Recycle(heap);\n+}\n+\n+bool Heap_shouldGrow(Heap *heap) {\n+    uint32_t freeBlockCount = (uint32_t)blockAllocator.freeBlockCount;\n+    uint32_t blockCount = heap->blockCount;\n+    uint32_t recycledBlockCount = (uint32_t)allocator.recycledBlockCount;\n+    uint32_t unavailableBlockCount =\n+        blockCount - (freeBlockCount + recycledBlockCount);\n+\n+#ifdef DEBUG_PRINT\n+    printf(\"\\n\\nBlock count: %\" PRIu32 \"\\n\", blockCount);\n+    printf(\"Unavailable: %\" PRIu32 \"\\n\", unavailableBlockCount);\n+    printf(\"Free: %\" PRIu32 \"\\n\", freeBlockCount);\n+    printf(\"Recycled: %\" PRIu32 \"\\n\", recycledBlockCount);\n+    fflush(stdout);\n+#endif\n+\n+    uint64_t timeInMark = heap->mark.currentEnd_ns - heap->mark.currentStart_ns;\n+    uint64_t timeTotal = heap->mark.currentEnd_ns - heap->mark.lastEnd_ns;\n+\n+    return timeInMark >= GROWTH_MARK_FRACTION * timeTotal || freeBlockCount * 2 < blockCount ||\n+           4 * unavailableBlockCount > blockCount;\n+}\n+\n+void Heap_Recycle(Heap *heap) {\n+    Allocator_Clear(&allocator);\n+    LargeAllocator_Clear(&largeAllocator);\n+    BlockAllocator_Clear(&blockAllocator);\n+\n+    // use the reserved block so mutator can does not have to lazy sweep\n+    // but can allocate imminently\n+    BlockAllocator_UseReserve(&blockAllocator);\n+\n+    // all the marking changes should be visible to all threads by now\n+    atomic_thread_fence(memory_order_seq_cst);\n+\n+    // before changing the cursor and limit values, makes sure no gc threads are\n+    // running\n+    GCThread_JoinAll(heap);"
  },
  {
    "id" : "e9a9915a-0112-4e45-b44a-e7a3043a2e97",
    "prId" : 1423,
    "comments" : [
      {
        "id" : "5f74c3eb-e9a7-4549-ad00-90da6b553550",
        "parentId" : null,
        "author" : {
          "login" : "densh",
          "name" : null,
          "avatarUrl" : "https://avatars1.githubusercontent.com/u/320966?u=784f6f761f35b8b7f3f787172b468334d6524524&v=4"
        },
        "body" : "Free block fraction is 50%, lets make it a publicly configurable setting, similarly to heap size. ",
        "createdAt" : "2019-02-08T11:45:42Z",
        "updatedAt" : "2019-07-17T09:53:34Z",
        "lastEditedBy" : {
          "login" : "densh",
          "name" : null,
          "avatarUrl" : "https://avatars1.githubusercontent.com/u/320966?u=784f6f761f35b8b7f3f787172b468334d6524524&v=4"
        },
        "tags" : [
          {
            "value" : "outdated"
          }
        ]
      },
      {
        "id" : "5891c557-d8fc-4c7d-badb-52137670850d",
        "parentId" : "5f74c3eb-e9a7-4549-ad00-90da6b553550",
        "author" : {
          "login" : "densh",
          "name" : null,
          "avatarUrl" : "https://avatars1.githubusercontent.com/u/320966?u=784f6f761f35b8b7f3f787172b468334d6524524&v=4"
        },
        "body" : "i.e., `SCALANATIVE_FREE_RATIO`.",
        "createdAt" : "2019-02-08T12:01:29Z",
        "updatedAt" : "2019-07-17T09:53:34Z",
        "lastEditedBy" : {
          "login" : "densh",
          "name" : null,
          "avatarUrl" : "https://avatars1.githubusercontent.com/u/320966?u=784f6f761f35b8b7f3f787172b468334d6524524&v=4"
        },
        "tags" : [
          {
            "value" : "outdated"
          }
        ]
      },
      {
        "id" : "572a618a-a1e8-4d8d-ae4f-1077f1fea0f4",
        "parentId" : "5f74c3eb-e9a7-4549-ad00-90da6b553550",
        "author" : {
          "login" : "valdisxp1",
          "name" : null,
          "avatarUrl" : "https://avatars3.githubusercontent.com/u/2277076?v=4"
        },
        "body" : "Done",
        "createdAt" : "2019-02-27T14:26:02Z",
        "updatedAt" : "2019-07-17T09:53:34Z",
        "lastEditedBy" : {
          "login" : "valdisxp1",
          "name" : null,
          "avatarUrl" : "https://avatars3.githubusercontent.com/u/2277076?v=4"
        },
        "tags" : [
          {
            "value" : "outdated"
          }
        ]
      }
    ],
    "commit" : "9349177c2456e08c91598c5a158c63a30876a4e0",
    "line" : null,
    "diffHunk" : "@@ -0,0 +1,308 @@\n+#include <stdlib.h>\n+#include <sys/mman.h>\n+#include <stdio.h>\n+#include \"Heap.h\"\n+#include \"Log.h\"\n+#include \"Allocator.h\"\n+#include \"LargeAllocator.h\"\n+#include \"Marker.h\"\n+#include \"State.h\"\n+#include \"utils/MathUtils.h\"\n+#include \"StackTrace.h\"\n+#include \"Settings.h\"\n+#include \"Memory.h\"\n+#include \"GCThread.h\"\n+#include \"Sweeper.h\"\n+#include \"Phase.h\"\n+#include <memory.h>\n+#include <time.h>\n+#include <inttypes.h>\n+\n+// Allow read and write\n+#define HEAP_MEM_PROT (PROT_READ | PROT_WRITE)\n+// Map private anonymous memory, and prevent from reserving swap\n+#define HEAP_MEM_FLAGS (MAP_NORESERVE | MAP_PRIVATE | MAP_ANONYMOUS)\n+// Map anonymous memory (not a file)\n+#define HEAP_MEM_FD -1\n+#define HEAP_MEM_FD_OFFSET 0\n+\n+void Heap_exitWithOutOfMemory() {\n+    printf(\"Out of heap space\\n\");\n+    StackTrace_PrintStackTrace();\n+    exit(1);\n+}\n+\n+bool Heap_isGrowingPossible(Heap *heap, uint32_t incrementInBlocks) {\n+    return heap->blockCount + incrementInBlocks <= heap->maxBlockCount;\n+}\n+\n+size_t Heap_getMemoryLimit() {\n+    size_t memorySize = getMemorySize();\n+    if ((uint64_t)memorySize > MAX_HEAP_SIZE) {\n+        return (size_t)MAX_HEAP_SIZE;\n+    } else {\n+        return memorySize;\n+    }\n+}\n+\n+/**\n+ * Maps `MAX_SIZE` of memory and returns the first address aligned on\n+ * `alignement` mask\n+ */\n+word_t *Heap_mapAndAlign(size_t memoryLimit, size_t alignmentSize) {\n+    assert(alignmentSize % WORD_SIZE == 0);\n+    word_t *heapStart = mmap(NULL, memoryLimit, HEAP_MEM_PROT, HEAP_MEM_FLAGS,\n+                             HEAP_MEM_FD, HEAP_MEM_FD_OFFSET);\n+\n+    size_t alignmentMask = ~(alignmentSize - 1);\n+    // Heap start not aligned on\n+    if (((word_t)heapStart & alignmentMask) != (word_t)heapStart) {\n+        word_t *previousBlock = (word_t *)((word_t)heapStart & alignmentMask);\n+        heapStart = previousBlock + alignmentSize / WORD_SIZE;\n+    }\n+    return heapStart;\n+}\n+\n+#ifdef ENABLE_GC_STATS\n+INLINE Stats *Heap_createMutatorStats(void) {\n+    char *statsFile = Settings_StatsFileName();\n+    if (statsFile != NULL) {\n+        Stats *stats = malloc(sizeof(Stats));\n+        Stats_Init(stats, statsFile, MUTATOR_THREAD_ID);\n+        return stats;\n+    } else {\n+        return NULL;\n+    }\n+}\n+\n+INLINE Stats *Heap_createStatsForThread(int id) {\n+    char *statsFile = Settings_StatsFileName();\n+    if (statsFile != NULL) {\n+        int len = strlen(statsFile) + 5;\n+        char *threadSpecificFile = (char *)malloc(len);\n+        snprintf(threadSpecificFile, len, \"%s.t%d\", statsFile, id);\n+        Stats *stats = malloc(sizeof(Stats));\n+        Stats_Init(stats, threadSpecificFile, (uint8_t)id);\n+        return stats;\n+    } else {\n+        return NULL;\n+    }\n+}\n+\n+#endif\n+\n+/**\n+ * Allocates the heap struct and initializes it\n+ */\n+void Heap_Init(Heap *heap, size_t minHeapSize, size_t maxHeapSize) {\n+    size_t memoryLimit = Heap_getMemoryLimit();\n+\n+    if (maxHeapSize < MIN_HEAP_SIZE) {\n+        fprintf(stderr,\n+                \"SCALANATIVE_MAX_HEAP_SIZE too small to initialize heap.\\n\");\n+        fprintf(stderr, \"Minimum required: %zum \\n\",\n+                MIN_HEAP_SIZE / 1024 / 1024);\n+        fflush(stderr);\n+        exit(1);\n+    }\n+\n+    if (minHeapSize > memoryLimit) {\n+        fprintf(stderr, \"SCALANATIVE_MIN_HEAP_SIZE is too large.\\n\");\n+        fprintf(stderr, \"Maximum possible: %zug \\n\",\n+                memoryLimit / 1024 / 1024 / 1024);\n+        fflush(stderr);\n+        exit(1);\n+    }\n+\n+    if (maxHeapSize < minHeapSize) {\n+        fprintf(stderr, \"SCALANATIVE_MAX_HEAP_SIZE should be at least \"\n+                        \"SCALANATIVE_MIN_HEAP_SIZE\\n\");\n+        fflush(stderr);\n+        exit(1);\n+    }\n+\n+    if (minHeapSize < MIN_HEAP_SIZE) {\n+        minHeapSize = MIN_HEAP_SIZE;\n+    }\n+\n+    if (maxHeapSize == UNLIMITED_HEAP_SIZE) {\n+        maxHeapSize = memoryLimit;\n+    }\n+\n+    uint32_t maxNumberOfBlocks = maxHeapSize / SPACE_USED_PER_BLOCK;\n+    uint32_t initialBlockCount = minHeapSize / SPACE_USED_PER_BLOCK;\n+    heap->maxHeapSize = maxHeapSize;\n+    heap->blockCount = initialBlockCount;\n+    heap->maxBlockCount = maxNumberOfBlocks;\n+\n+    // reserve space for block headers\n+    size_t blockMetaSpaceSize = maxNumberOfBlocks * sizeof(BlockMeta);\n+    word_t *blockMetaStart = Heap_mapAndAlign(blockMetaSpaceSize, WORD_SIZE);\n+    heap->blockMetaStart = blockMetaStart;\n+    heap->blockMetaEnd =\n+        blockMetaStart + initialBlockCount * sizeof(BlockMeta) / WORD_SIZE;\n+\n+    // reserve space for line headers\n+    size_t lineMetaSpaceSize =\n+        (size_t)maxNumberOfBlocks * LINE_COUNT * LINE_METADATA_SIZE;\n+    word_t *lineMetaStart = Heap_mapAndAlign(lineMetaSpaceSize, WORD_SIZE);\n+    heap->lineMetaStart = lineMetaStart;\n+    assert(LINE_COUNT * LINE_SIZE == BLOCK_TOTAL_SIZE);\n+    assert(LINE_COUNT * LINE_METADATA_SIZE % WORD_SIZE == 0);\n+    heap->lineMetaEnd = lineMetaStart + initialBlockCount * LINE_COUNT *\n+                                            LINE_METADATA_SIZE / WORD_SIZE;\n+\n+    word_t *heapStart = Heap_mapAndAlign(maxHeapSize, BLOCK_TOTAL_SIZE);\n+\n+    BlockAllocator_Init(&blockAllocator, blockMetaStart, initialBlockCount);\n+    GreyList_Init(&heap->mark.empty);\n+    GreyList_Init(&heap->mark.full);\n+    uint32_t greyPacketCount =\n+        (uint32_t)(maxHeapSize * GREY_PACKET_RATIO / GREY_PACKET_SIZE);\n+    heap->mark.total = greyPacketCount;\n+    word_t *greyPacketsStart =\n+        Heap_mapAndAlign(greyPacketCount * sizeof(GreyPacket), WORD_SIZE);\n+    heap->greyPacketsStart = greyPacketsStart;\n+    GreyList_PushAll(&heap->mark.empty, greyPacketsStart,\n+                     (GreyPacket *)greyPacketsStart, greyPacketCount);\n+\n+    // reserve space for bytemap\n+    Bytemap *bytemap = (Bytemap *)Heap_mapAndAlign(\n+        maxHeapSize / ALLOCATION_ALIGNMENT + sizeof(Bytemap),\n+        ALLOCATION_ALIGNMENT);\n+    heap->bytemap = bytemap;\n+\n+    // Init heap for small objects\n+    heap->heapSize = minHeapSize;\n+    heap->heapStart = heapStart;\n+    heap->heapEnd = heapStart + minHeapSize / WORD_SIZE;\n+\n+    Phase_Init(heap, initialBlockCount);\n+\n+    Bytemap_Init(bytemap, heapStart, maxHeapSize);\n+    Allocator_Init(&allocator, &blockAllocator, bytemap, blockMetaStart,\n+                   heapStart);\n+\n+    LargeAllocator_Init(&largeAllocator, &blockAllocator, bytemap,\n+                        blockMetaStart, heapStart);\n+\n+    // Init all GCThreads\n+    // Init stats if enabled.\n+    // This must done before initializing other threads.\n+    heap->stats = Stats_OrNull(Heap_createMutatorStats());\n+\n+    int gcThreadCount = Settings_GCThreadCount();\n+    heap->gcThreads.count = gcThreadCount;\n+    Phase_Set(heap, gc_idle);\n+    GCThread *gcThreads = (GCThread *)malloc(sizeof(GCThread) * gcThreadCount);\n+    heap->gcThreads.all = (void *)gcThreads;\n+    for (int i = 0; i < gcThreadCount; i++) {\n+        Stats *stats = Stats_OrNull(Heap_createStatsForThread(i));\n+        GCThread_Init(&gcThreads[i], i, heap, stats);\n+    }\n+\n+    heap->mark.lastEnd_ns = scalanative_nano_time();\n+\n+    pthread_mutex_init(&heap->sweep.growMutex, NULL);\n+}\n+\n+void Heap_Collect(Heap *heap) {\n+    Stats *stats = Stats_OrNull(heap->stats);\n+    Stats_CollectionStarted(stats);\n+    assert(Sweeper_IsSweepDone(heap));\n+#ifdef DEBUG_ASSERT\n+    Sweeper_ClearIsSwept(heap);\n+    Sweeper_AssertIsConsistent(heap);\n+#endif\n+    Phase_StartMark(heap);\n+    Marker_MarkRoots(heap, stats);\n+    Marker_MarkUtilDone(heap, stats);\n+    Phase_MarkDone(heap);\n+    Stats_RecordEvent(stats, event_mark, heap->mark.currentStart_ns,\n+                      heap->mark.currentEnd_ns);\n+    Phase_StartSweep(heap);\n+}\n+\n+bool Heap_shouldGrow(Heap *heap) {\n+    uint32_t freeBlockCount = (uint32_t)blockAllocator.freeBlockCount;\n+    uint32_t blockCount = heap->blockCount;\n+    uint32_t recycledBlockCount = (uint32_t)allocator.recycledBlockCount;\n+    uint32_t unavailableBlockCount =\n+        blockCount - (freeBlockCount + recycledBlockCount);\n+\n+#ifdef DEBUG_PRINT\n+    printf(\"\\n\\nBlock count: %\" PRIu32 \"\\n\", blockCount);\n+    printf(\"Unavailable: %\" PRIu32 \"\\n\", unavailableBlockCount);\n+    printf(\"Free: %\" PRIu32 \"\\n\", freeBlockCount);\n+    printf(\"Recycled: %\" PRIu32 \"\\n\", recycledBlockCount);\n+    fflush(stdout);\n+#endif\n+\n+    uint64_t timeInMark = heap->mark.currentEnd_ns - heap->mark.currentStart_ns;\n+    uint64_t timeTotal = heap->mark.currentEnd_ns - heap->mark.lastEnd_ns;\n+\n+    return timeInMark >= GROWTH_MARK_FRACTION * timeTotal ||\n+           freeBlockCount * 2 < blockCount ||"
  },
  {
    "id" : "6b13259d-0eb1-410f-9369-b0f49359b8e6",
    "prId" : 1423,
    "comments" : [
      {
        "id" : "257081d5-0546-47df-bfac-4b9b62a2e4d3",
        "parentId" : null,
        "author" : {
          "login" : "densh",
          "name" : null,
          "avatarUrl" : "https://avatars1.githubusercontent.com/u/320966?u=784f6f761f35b8b7f3f787172b468334d6524524&v=4"
        },
        "body" : "This one (a fraction of unavailable block count) should be a constant in Constants.h ",
        "createdAt" : "2019-02-08T11:47:02Z",
        "updatedAt" : "2019-07-17T09:53:34Z",
        "lastEditedBy" : {
          "login" : "densh",
          "name" : null,
          "avatarUrl" : "https://avatars1.githubusercontent.com/u/320966?u=784f6f761f35b8b7f3f787172b468334d6524524&v=4"
        },
        "tags" : [
          {
            "value" : "outdated"
          }
        ]
      },
      {
        "id" : "1bd38e45-4a85-4e63-b359-731e61815d88",
        "parentId" : "257081d5-0546-47df-bfac-4b9b62a2e4d3",
        "author" : {
          "login" : "valdisxp1",
          "name" : null,
          "avatarUrl" : "https://avatars3.githubusercontent.com/u/2277076?v=4"
        },
        "body" : "Done",
        "createdAt" : "2019-02-27T14:25:54Z",
        "updatedAt" : "2019-07-17T09:53:34Z",
        "lastEditedBy" : {
          "login" : "valdisxp1",
          "name" : null,
          "avatarUrl" : "https://avatars3.githubusercontent.com/u/2277076?v=4"
        },
        "tags" : [
          {
            "value" : "outdated"
          }
        ]
      }
    ],
    "commit" : "9349177c2456e08c91598c5a158c63a30876a4e0",
    "line" : null,
    "diffHunk" : "@@ -0,0 +1,308 @@\n+#include <stdlib.h>\n+#include <sys/mman.h>\n+#include <stdio.h>\n+#include \"Heap.h\"\n+#include \"Log.h\"\n+#include \"Allocator.h\"\n+#include \"LargeAllocator.h\"\n+#include \"Marker.h\"\n+#include \"State.h\"\n+#include \"utils/MathUtils.h\"\n+#include \"StackTrace.h\"\n+#include \"Settings.h\"\n+#include \"Memory.h\"\n+#include \"GCThread.h\"\n+#include \"Sweeper.h\"\n+#include \"Phase.h\"\n+#include <memory.h>\n+#include <time.h>\n+#include <inttypes.h>\n+\n+// Allow read and write\n+#define HEAP_MEM_PROT (PROT_READ | PROT_WRITE)\n+// Map private anonymous memory, and prevent from reserving swap\n+#define HEAP_MEM_FLAGS (MAP_NORESERVE | MAP_PRIVATE | MAP_ANONYMOUS)\n+// Map anonymous memory (not a file)\n+#define HEAP_MEM_FD -1\n+#define HEAP_MEM_FD_OFFSET 0\n+\n+void Heap_exitWithOutOfMemory() {\n+    printf(\"Out of heap space\\n\");\n+    StackTrace_PrintStackTrace();\n+    exit(1);\n+}\n+\n+bool Heap_isGrowingPossible(Heap *heap, uint32_t incrementInBlocks) {\n+    return heap->blockCount + incrementInBlocks <= heap->maxBlockCount;\n+}\n+\n+size_t Heap_getMemoryLimit() {\n+    size_t memorySize = getMemorySize();\n+    if ((uint64_t)memorySize > MAX_HEAP_SIZE) {\n+        return (size_t)MAX_HEAP_SIZE;\n+    } else {\n+        return memorySize;\n+    }\n+}\n+\n+/**\n+ * Maps `MAX_SIZE` of memory and returns the first address aligned on\n+ * `alignement` mask\n+ */\n+word_t *Heap_mapAndAlign(size_t memoryLimit, size_t alignmentSize) {\n+    assert(alignmentSize % WORD_SIZE == 0);\n+    word_t *heapStart = mmap(NULL, memoryLimit, HEAP_MEM_PROT, HEAP_MEM_FLAGS,\n+                             HEAP_MEM_FD, HEAP_MEM_FD_OFFSET);\n+\n+    size_t alignmentMask = ~(alignmentSize - 1);\n+    // Heap start not aligned on\n+    if (((word_t)heapStart & alignmentMask) != (word_t)heapStart) {\n+        word_t *previousBlock = (word_t *)((word_t)heapStart & alignmentMask);\n+        heapStart = previousBlock + alignmentSize / WORD_SIZE;\n+    }\n+    return heapStart;\n+}\n+\n+#ifdef ENABLE_GC_STATS\n+INLINE Stats *Heap_createMutatorStats(void) {\n+    char *statsFile = Settings_StatsFileName();\n+    if (statsFile != NULL) {\n+        Stats *stats = malloc(sizeof(Stats));\n+        Stats_Init(stats, statsFile, MUTATOR_THREAD_ID);\n+        return stats;\n+    } else {\n+        return NULL;\n+    }\n+}\n+\n+INLINE Stats *Heap_createStatsForThread(int id) {\n+    char *statsFile = Settings_StatsFileName();\n+    if (statsFile != NULL) {\n+        int len = strlen(statsFile) + 5;\n+        char *threadSpecificFile = (char *)malloc(len);\n+        snprintf(threadSpecificFile, len, \"%s.t%d\", statsFile, id);\n+        Stats *stats = malloc(sizeof(Stats));\n+        Stats_Init(stats, threadSpecificFile, (uint8_t)id);\n+        return stats;\n+    } else {\n+        return NULL;\n+    }\n+}\n+\n+#endif\n+\n+/**\n+ * Allocates the heap struct and initializes it\n+ */\n+void Heap_Init(Heap *heap, size_t minHeapSize, size_t maxHeapSize) {\n+    size_t memoryLimit = Heap_getMemoryLimit();\n+\n+    if (maxHeapSize < MIN_HEAP_SIZE) {\n+        fprintf(stderr,\n+                \"SCALANATIVE_MAX_HEAP_SIZE too small to initialize heap.\\n\");\n+        fprintf(stderr, \"Minimum required: %zum \\n\",\n+                MIN_HEAP_SIZE / 1024 / 1024);\n+        fflush(stderr);\n+        exit(1);\n+    }\n+\n+    if (minHeapSize > memoryLimit) {\n+        fprintf(stderr, \"SCALANATIVE_MIN_HEAP_SIZE is too large.\\n\");\n+        fprintf(stderr, \"Maximum possible: %zug \\n\",\n+                memoryLimit / 1024 / 1024 / 1024);\n+        fflush(stderr);\n+        exit(1);\n+    }\n+\n+    if (maxHeapSize < minHeapSize) {\n+        fprintf(stderr, \"SCALANATIVE_MAX_HEAP_SIZE should be at least \"\n+                        \"SCALANATIVE_MIN_HEAP_SIZE\\n\");\n+        fflush(stderr);\n+        exit(1);\n+    }\n+\n+    if (minHeapSize < MIN_HEAP_SIZE) {\n+        minHeapSize = MIN_HEAP_SIZE;\n+    }\n+\n+    if (maxHeapSize == UNLIMITED_HEAP_SIZE) {\n+        maxHeapSize = memoryLimit;\n+    }\n+\n+    uint32_t maxNumberOfBlocks = maxHeapSize / SPACE_USED_PER_BLOCK;\n+    uint32_t initialBlockCount = minHeapSize / SPACE_USED_PER_BLOCK;\n+    heap->maxHeapSize = maxHeapSize;\n+    heap->blockCount = initialBlockCount;\n+    heap->maxBlockCount = maxNumberOfBlocks;\n+\n+    // reserve space for block headers\n+    size_t blockMetaSpaceSize = maxNumberOfBlocks * sizeof(BlockMeta);\n+    word_t *blockMetaStart = Heap_mapAndAlign(blockMetaSpaceSize, WORD_SIZE);\n+    heap->blockMetaStart = blockMetaStart;\n+    heap->blockMetaEnd =\n+        blockMetaStart + initialBlockCount * sizeof(BlockMeta) / WORD_SIZE;\n+\n+    // reserve space for line headers\n+    size_t lineMetaSpaceSize =\n+        (size_t)maxNumberOfBlocks * LINE_COUNT * LINE_METADATA_SIZE;\n+    word_t *lineMetaStart = Heap_mapAndAlign(lineMetaSpaceSize, WORD_SIZE);\n+    heap->lineMetaStart = lineMetaStart;\n+    assert(LINE_COUNT * LINE_SIZE == BLOCK_TOTAL_SIZE);\n+    assert(LINE_COUNT * LINE_METADATA_SIZE % WORD_SIZE == 0);\n+    heap->lineMetaEnd = lineMetaStart + initialBlockCount * LINE_COUNT *\n+                                            LINE_METADATA_SIZE / WORD_SIZE;\n+\n+    word_t *heapStart = Heap_mapAndAlign(maxHeapSize, BLOCK_TOTAL_SIZE);\n+\n+    BlockAllocator_Init(&blockAllocator, blockMetaStart, initialBlockCount);\n+    GreyList_Init(&heap->mark.empty);\n+    GreyList_Init(&heap->mark.full);\n+    uint32_t greyPacketCount =\n+        (uint32_t)(maxHeapSize * GREY_PACKET_RATIO / GREY_PACKET_SIZE);\n+    heap->mark.total = greyPacketCount;\n+    word_t *greyPacketsStart =\n+        Heap_mapAndAlign(greyPacketCount * sizeof(GreyPacket), WORD_SIZE);\n+    heap->greyPacketsStart = greyPacketsStart;\n+    GreyList_PushAll(&heap->mark.empty, greyPacketsStart,\n+                     (GreyPacket *)greyPacketsStart, greyPacketCount);\n+\n+    // reserve space for bytemap\n+    Bytemap *bytemap = (Bytemap *)Heap_mapAndAlign(\n+        maxHeapSize / ALLOCATION_ALIGNMENT + sizeof(Bytemap),\n+        ALLOCATION_ALIGNMENT);\n+    heap->bytemap = bytemap;\n+\n+    // Init heap for small objects\n+    heap->heapSize = minHeapSize;\n+    heap->heapStart = heapStart;\n+    heap->heapEnd = heapStart + minHeapSize / WORD_SIZE;\n+\n+    Phase_Init(heap, initialBlockCount);\n+\n+    Bytemap_Init(bytemap, heapStart, maxHeapSize);\n+    Allocator_Init(&allocator, &blockAllocator, bytemap, blockMetaStart,\n+                   heapStart);\n+\n+    LargeAllocator_Init(&largeAllocator, &blockAllocator, bytemap,\n+                        blockMetaStart, heapStart);\n+\n+    // Init all GCThreads\n+    // Init stats if enabled.\n+    // This must done before initializing other threads.\n+    heap->stats = Stats_OrNull(Heap_createMutatorStats());\n+\n+    int gcThreadCount = Settings_GCThreadCount();\n+    heap->gcThreads.count = gcThreadCount;\n+    Phase_Set(heap, gc_idle);\n+    GCThread *gcThreads = (GCThread *)malloc(sizeof(GCThread) * gcThreadCount);\n+    heap->gcThreads.all = (void *)gcThreads;\n+    for (int i = 0; i < gcThreadCount; i++) {\n+        Stats *stats = Stats_OrNull(Heap_createStatsForThread(i));\n+        GCThread_Init(&gcThreads[i], i, heap, stats);\n+    }\n+\n+    heap->mark.lastEnd_ns = scalanative_nano_time();\n+\n+    pthread_mutex_init(&heap->sweep.growMutex, NULL);\n+}\n+\n+void Heap_Collect(Heap *heap) {\n+    Stats *stats = Stats_OrNull(heap->stats);\n+    Stats_CollectionStarted(stats);\n+    assert(Sweeper_IsSweepDone(heap));\n+#ifdef DEBUG_ASSERT\n+    Sweeper_ClearIsSwept(heap);\n+    Sweeper_AssertIsConsistent(heap);\n+#endif\n+    Phase_StartMark(heap);\n+    Marker_MarkRoots(heap, stats);\n+    Marker_MarkUtilDone(heap, stats);\n+    Phase_MarkDone(heap);\n+    Stats_RecordEvent(stats, event_mark, heap->mark.currentStart_ns,\n+                      heap->mark.currentEnd_ns);\n+    Phase_StartSweep(heap);\n+}\n+\n+bool Heap_shouldGrow(Heap *heap) {\n+    uint32_t freeBlockCount = (uint32_t)blockAllocator.freeBlockCount;\n+    uint32_t blockCount = heap->blockCount;\n+    uint32_t recycledBlockCount = (uint32_t)allocator.recycledBlockCount;\n+    uint32_t unavailableBlockCount =\n+        blockCount - (freeBlockCount + recycledBlockCount);\n+\n+#ifdef DEBUG_PRINT\n+    printf(\"\\n\\nBlock count: %\" PRIu32 \"\\n\", blockCount);\n+    printf(\"Unavailable: %\" PRIu32 \"\\n\", unavailableBlockCount);\n+    printf(\"Free: %\" PRIu32 \"\\n\", freeBlockCount);\n+    printf(\"Recycled: %\" PRIu32 \"\\n\", recycledBlockCount);\n+    fflush(stdout);\n+#endif\n+\n+    uint64_t timeInMark = heap->mark.currentEnd_ns - heap->mark.currentStart_ns;\n+    uint64_t timeTotal = heap->mark.currentEnd_ns - heap->mark.lastEnd_ns;\n+\n+    return timeInMark >= GROWTH_MARK_FRACTION * timeTotal ||\n+           freeBlockCount * 2 < blockCount ||\n+           4 * unavailableBlockCount > blockCount;"
  }
]